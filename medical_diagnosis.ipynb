{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimadadda/Deep_learning_project/blob/main/medical_diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1XhGFNdRSebN",
        "outputId": "8d1ff19a-99e7-4e20-e4bc-4cbeab1e17a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " epoch: 0  auc: 0.40541252965468766\n",
            " epoch: 1  auc: 0.41477023108689914\n",
            " epoch: 2  auc: 0.41696687461558735\n",
            " epoch: 3  auc: 0.42263421491960285\n",
            " epoch: 4  auc: 0.42636850891837275\n",
            " epoch: 5  auc: 0.4330463052455848\n",
            " epoch: 6  auc: 0.43950443721992793\n",
            " epoch: 7  auc: 0.44367805992443543\n",
            " epoch: 8  auc: 0.4464897636411563\n",
            " epoch: 9  auc: 0.4500043932870574\n",
            " epoch: 10  auc: 0.45184957385115543\n",
            " epoch: 11  auc: 0.45523240488533523\n",
            " epoch: 12  auc: 0.4622177313065635\n",
            " epoch: 13  auc: 0.4718390299622177\n",
            " epoch: 14  auc: 0.48110886565328176\n",
            " epoch: 15  auc: 0.4939372638608206\n",
            " epoch: 16  auc: 0.5080836481855724\n",
            " epoch: 17  auc: 0.5228450926983569\n",
            " epoch: 18  auc: 0.5406379052807312\n",
            " epoch: 19  auc: 0.5587821808276953\n",
            " epoch: 20  auc: 0.5773218522098235\n",
            " epoch: 21  auc: 0.5977506370266233\n",
            " epoch: 22  auc: 0.6188384149020297\n",
            " epoch: 23  auc: 0.6395307969422722\n",
            " epoch: 24  auc: 0.6609920042175556\n",
            " epoch: 25  auc: 0.6806519637993146\n",
            " epoch: 26  auc: 0.6977857833230824\n",
            " epoch: 27  auc: 0.7136894824707847\n",
            " epoch: 28  auc: 0.7283630612424216\n",
            " epoch: 29  auc: 0.7413232580616818\n",
            " epoch: 30  auc: 0.7539319919163517\n",
            " epoch: 31  auc: 0.7656181354889728\n",
            " epoch: 32  auc: 0.7756787628503646\n",
            " epoch: 33  auc: 0.7825762235304455\n",
            " epoch: 34  auc: 0.7878921008698708\n",
            " epoch: 35  auc: 0.790835603198313\n",
            " epoch: 36  auc: 0.7937791055267551\n",
            " epoch: 37  auc: 0.7929883138564273\n",
            " epoch: 38  auc: 0.7948334944205254\n",
            " epoch: 39  auc: 0.794174501361919\n",
            " epoch: 40  auc: 0.7936473069150338\n",
            " epoch: 41  auc: 0.7919778578332308\n",
            " epoch: 42  auc: 0.791011334680608\n",
            " epoch: 43  auc: 0.7909234689394605\n",
            " epoch: 44  auc: 0.7896933485633952\n",
            " epoch: 45  auc: 0.7871232756348299\n",
            " epoch: 46  auc: 0.7859590545646253\n",
            " epoch: 47  auc: 0.7854318601177401\n",
            " epoch: 48  auc: 0.7842456726122484\n",
            " epoch: 49  auc: 0.7825322906598718\n",
            " epoch: 50  auc: 0.7821808276952815\n",
            " epoch: 51  auc: 0.7814339688955276\n",
            " epoch: 52  auc: 0.7803795800017572\n",
            " epoch: 53  auc: 0.7797645198137246\n",
            " epoch: 54  auc: 0.7791055267551181\n",
            " epoch: 55  auc: 0.777853439943766\n",
            " epoch: 56  auc: 0.7761180915561022\n",
            " epoch: 57  auc: 0.7747561725683156\n",
            " epoch: 58  auc: 0.7742289781214304\n",
            " epoch: 59  auc: 0.7729109920042174\n",
            " epoch: 60  auc: 0.7717248044987259\n",
            " epoch: 61  auc: 0.7715930058870046\n",
            " epoch: 62  auc: 0.7711097443106932\n",
            " epoch: 63  auc: 0.7702750197697917\n",
            " epoch: 64  auc: 0.7700992882874967\n",
            " epoch: 65  auc: 0.7693963623583164\n",
            " epoch: 66  auc: 0.7686934364291362\n",
            " epoch: 67  auc: 0.7685177049468412\n",
            " epoch: 68  auc: 0.7675072489236446\n",
            " epoch: 69  auc: 0.7672436517002021\n",
            " epoch: 70  auc: 0.7666725243827432\n",
            " epoch: 71  auc: 0.7665846586415956\n",
            " epoch: 72  auc: 0.7663649942887267\n",
            " epoch: 73  auc: 0.7661013970652842\n",
            " epoch: 74  auc: 0.7652007732185222\n",
            " epoch: 75  auc: 0.7648712766892188\n",
            " epoch: 76  auc: 0.764124417889465\n",
            " epoch: 77  auc: 0.7638168877954485\n",
            " epoch: 78  auc: 0.7636850891837271\n",
            " epoch: 79  auc: 0.7636850891837274\n",
            " epoch: 80  auc: 0.7640804850188911\n",
            " epoch: 81  auc: 0.7636411563131534\n",
            " epoch: 82  auc: 0.7635093577014321\n",
            " epoch: 83  auc: 0.7634654248308586\n",
            " epoch: 84  auc: 0.7636850891837272\n",
            " epoch: 85  auc: 0.7633775590897108\n",
            " epoch: 86  auc: 0.7631139618662683\n",
            " epoch: 87  auc: 0.7630700289956945\n",
            " epoch: 88  auc: 0.7630260961251208\n",
            " epoch: 89  auc: 0.7621913715842193\n",
            " epoch: 90  auc: 0.7621474387136455\n",
            " epoch: 91  auc: 0.7618838414902029\n",
            " epoch: 92  auc: 0.7619277743607767\n",
            " epoch: 93  auc: 0.7620595729724979\n",
            " epoch: 94  auc: 0.7623231701959406\n",
            " epoch: 95  auc: 0.7621035058430717\n",
            " epoch: 96  auc: 0.7618838414902029\n",
            " epoch: 97  auc: 0.7617081100079078\n",
            " epoch: 98  auc: 0.7617520428784817\n",
            " epoch: 99  auc: 0.762059572972498\n",
            " epoch: 100  auc: 0.7623231701959406\n",
            " epoch: 101  auc: 0.7623231701959406\n",
            " epoch: 102  auc: 0.7623671030665143\n",
            " epoch: 103  auc: 0.7625428345488094\n",
            " epoch: 104  auc: 0.762454968807662\n",
            " epoch: 105  auc: 0.7626746331605307\n",
            " epoch: 106  auc: 0.7625867674193832\n",
            " epoch: 107  auc: 0.7624549688076618\n",
            " epoch: 108  auc: 0.7630260961251208\n",
            " epoch: 109  auc: 0.7632457604779896\n",
            " epoch: 110  auc: 0.7629382303839732\n",
            " epoch: 111  auc: 0.7632018276074158\n",
            " epoch: 112  auc: 0.7635093577014321\n",
            " epoch: 113  auc: 0.7634214919602846\n",
            " epoch: 114  auc: 0.7630260961251208\n",
            " epoch: 115  auc: 0.7634654248308584\n",
            " epoch: 116  auc: 0.7634214919602846\n",
            " epoch: 117  auc: 0.763157894736842\n",
            " epoch: 118  auc: 0.7632896933485634\n",
            " epoch: 119  auc: 0.7635093577014322\n",
            " epoch: 120  auc: 0.7634214919602846\n",
            " epoch: 121  auc: 0.7632896933485634\n",
            " epoch: 122  auc: 0.7632018276074158\n",
            " epoch: 123  auc: 0.7628503646428259\n",
            " epoch: 124  auc: 0.7632896933485633\n",
            " epoch: 125  auc: 0.7628942975133995\n",
            " epoch: 126  auc: 0.7624989016782356\n",
            " epoch: 127  auc: 0.7622353044547931\n",
            " epoch: 128  auc: 0.7624110359370881\n",
            " epoch: 129  auc: 0.7626746331605307\n",
            " epoch: 130  auc: 0.7626746331605306\n",
            " epoch: 131  auc: 0.7630260961251208\n",
            " epoch: 132  auc: 0.7632896933485634\n",
            " epoch: 133  auc: 0.7630260961251208\n",
            " epoch: 134  auc: 0.7630260961251207\n",
            " epoch: 135  auc: 0.7633775590897109\n",
            " epoch: 136  auc: 0.7632457604779896\n",
            " epoch: 137  auc: 0.7632896933485633\n",
            " epoch: 138  auc: 0.7631578947368421\n",
            "fold: 1  epoch: 38\n",
            "fold: 1  accuracy: 0.9403453689167975\n",
            "fold: 1  f1_score: 0.0\n",
            "fold: 1  auc: 0.7948334944205254\n",
            " epoch: 0  auc: 0.427810791537913\n",
            " epoch: 1  auc: 0.4227240313762776\n",
            " epoch: 2  auc: 0.4204421202757309\n",
            " epoch: 3  auc: 0.4165913952935584\n",
            " epoch: 4  auc: 0.4157832184454481\n",
            " epoch: 5  auc: 0.4138340860470644\n",
            " epoch: 6  auc: 0.4112193962443546\n",
            " epoch: 7  auc: 0.4128832897551699\n",
            " epoch: 8  auc: 0.413548847159496\n",
            " epoch: 9  auc: 0.4149750415973378\n",
            " epoch: 10  auc: 0.4154979795578797\n",
            " epoch: 11  auc: 0.4189208462086998\n",
            " epoch: 12  auc: 0.4226764915616829\n",
            " epoch: 13  auc: 0.42747801283574993\n",
            " epoch: 14  auc: 0.4330877109579273\n",
            " epoch: 15  auc: 0.4396006655574043\n",
            " epoch: 16  auc: 0.44725457570715477\n",
            " epoch: 17  auc: 0.45497979557879725\n",
            " epoch: 18  auc: 0.46512954599477063\n",
            " epoch: 19  auc: 0.47497028761587823\n",
            " epoch: 20  auc: 0.48607083432374615\n",
            " epoch: 21  auc: 0.49840741621107676\n",
            " epoch: 22  auc: 0.5110530068932732\n",
            " epoch: 23  auc: 0.5235084383170905\n",
            " epoch: 24  auc: 0.5356786308533397\n",
            " epoch: 25  auc: 0.5479439030187783\n",
            " epoch: 26  auc: 0.5612550511053007\n",
            " epoch: 27  auc: 0.5725695269788448\n",
            " epoch: 28  auc: 0.5832659852626575\n",
            " epoch: 29  auc: 0.5956263370572854\n",
            " epoch: 30  auc: 0.6077489897789399\n",
            " epoch: 31  auc: 0.6175897314000476\n",
            " epoch: 32  auc: 0.6267173758022343\n",
            " epoch: 33  auc: 0.6340385072498216\n",
            " epoch: 34  auc: 0.6420251961017351\n",
            " epoch: 35  auc: 0.6487283099595911\n",
            " epoch: 36  auc: 0.6542429284525789\n",
            " epoch: 37  auc: 0.6599001663893511\n",
            " epoch: 38  auc: 0.6643213691466604\n",
            " epoch: 39  auc: 0.6692655098645115\n",
            " epoch: 40  auc: 0.6740194913239839\n",
            " epoch: 41  auc: 0.6791300213929166\n",
            " epoch: 42  auc: 0.6832897551699548\n",
            " epoch: 43  auc: 0.6864749227478013\n",
            " epoch: 44  auc: 0.6900879486570002\n",
            " epoch: 45  auc: 0.6937485143807938\n",
            " epoch: 46  auc: 0.696648443071072\n",
            " epoch: 47  auc: 0.6995483717613502\n",
            " epoch: 48  auc: 0.7009745661991919\n",
            " epoch: 49  auc: 0.7036367958164964\n",
            " epoch: 50  auc: 0.7059187069170431\n",
            " epoch: 51  auc: 0.7079153791300213\n",
            " epoch: 52  auc: 0.7088661754219159\n",
            " epoch: 53  auc: 0.7112431661516521\n",
            " epoch: 54  auc: 0.7128119800332778\n",
            " epoch: 55  auc: 0.7139529355835512\n",
            " epoch: 56  auc: 0.7157594485381508\n",
            " epoch: 57  auc: 0.7166151652008557\n",
            " epoch: 58  auc: 0.7186593772284289\n",
            " epoch: 59  auc: 0.7188970763014025\n",
            " epoch: 60  auc: 0.7200380318516758\n",
            " epoch: 61  auc: 0.7206085096268126\n",
            " epoch: 62  auc: 0.7214642262895176\n",
            " epoch: 63  auc: 0.7219871642500594\n",
            " epoch: 64  auc: 0.7230330401711433\n",
            " epoch: 65  auc: 0.7238412170192535\n",
            " epoch: 66  auc: 0.7244592346089851\n",
            " epoch: 67  auc: 0.7249346327549322\n",
            " epoch: 68  auc: 0.7253149512716901\n",
            " epoch: 69  auc: 0.7256001901592584\n",
            " epoch: 70  auc: 0.7259805086760163\n",
            " epoch: 71  auc: 0.7264559068219634\n",
            " epoch: 72  auc: 0.7265985262657475\n",
            " epoch: 73  auc: 0.7266460660803423\n",
            " epoch: 74  auc: 0.7267411457095317\n",
            " epoch: 75  auc: 0.7270739244116948\n",
            " epoch: 76  auc: 0.7279771808889945\n",
            " epoch: 77  auc: 0.7282624197765628\n",
            " epoch: 78  auc: 0.7283574994057522\n",
            " epoch: 79  auc: 0.7289755169954837\n",
            " epoch: 80  auc: 0.7293082956976468\n",
            " epoch: 81  auc: 0.7295459947706204\n",
            " epoch: 82  auc: 0.7300689327311624\n",
            " epoch: 83  auc: 0.7309246493938674\n",
            " epoch: 84  auc: 0.731304967910625\n",
            " epoch: 85  auc: 0.7316377466127881\n",
            " epoch: 86  auc: 0.7317328262419777\n",
            " epoch: 87  auc: 0.7320656049441407\n",
            " epoch: 88  auc: 0.7328262419776563\n",
            " epoch: 89  auc: 0.7330164012360353\n",
            " epoch: 90  auc: 0.7331114808652246\n",
            " epoch: 91  auc: 0.7334442595673877\n",
            " epoch: 92  auc: 0.7335868790111718\n",
            " epoch: 93  auc: 0.7338245780841455\n",
            " epoch: 94  auc: 0.7343950558592821\n",
            " epoch: 95  auc: 0.7346802947468506\n",
            " epoch: 96  auc: 0.7347753743760399\n",
            " epoch: 97  auc: 0.7345376753030662\n",
            " epoch: 98  auc: 0.7348704540052293\n",
            " epoch: 99  auc: 0.73477537437604\n",
            " epoch: 100  auc: 0.7345852151176611\n",
            " epoch: 101  auc: 0.7346802947468505\n",
            " epoch: 102  auc: 0.7347753743760399\n",
            " epoch: 103  auc: 0.7348704540052294\n",
            " epoch: 104  auc: 0.7350606132636083\n",
            " epoch: 105  auc: 0.7350130734490136\n",
            " epoch: 106  auc: 0.7353458521511766\n",
            " epoch: 107  auc: 0.7355835512241502\n",
            " epoch: 108  auc: 0.7358687901117186\n",
            " epoch: 109  auc: 0.7355835512241502\n",
            " epoch: 110  auc: 0.7353458521511765\n",
            " epoch: 111  auc: 0.7353458521511765\n",
            " epoch: 112  auc: 0.7357737104825292\n",
            " epoch: 113  auc: 0.7357737104825292\n",
            " epoch: 114  auc: 0.7354409317803662\n",
            " epoch: 115  auc: 0.7356786308533397\n",
            " epoch: 116  auc: 0.7357737104825292\n",
            " epoch: 117  auc: 0.7358687901117186\n",
            " epoch: 118  auc: 0.735963869740908\n",
            " epoch: 119  auc: 0.7361064891846921\n",
            " epoch: 120  auc: 0.7358212502971239\n",
            " epoch: 121  auc: 0.7361064891846922\n",
            " epoch: 122  auc: 0.7362966484430712\n",
            " epoch: 123  auc: 0.7363441882576658\n",
            " epoch: 124  auc: 0.7364392678868552\n",
            " epoch: 125  auc: 0.7364392678868552\n",
            " epoch: 126  auc: 0.7361540289992868\n",
            " epoch: 127  auc: 0.7364392678868552\n",
            " epoch: 128  auc: 0.7366294271452342\n",
            " epoch: 129  auc: 0.7362966484430711\n",
            " epoch: 130  auc: 0.7362966484430711\n",
            " epoch: 131  auc: 0.7363441882576658\n",
            " epoch: 132  auc: 0.7364392678868553\n",
            " epoch: 133  auc: 0.7367245067744236\n",
            " epoch: 134  auc: 0.736676966959829\n",
            " epoch: 135  auc: 0.737009745661992\n",
            " epoch: 136  auc: 0.7369146660328025\n",
            " epoch: 137  auc: 0.7370097456619918\n",
            " epoch: 138  auc: 0.7366769669598289\n",
            " epoch: 139  auc: 0.7366769669598289\n",
            " epoch: 140  auc: 0.7368671262182077\n",
            " epoch: 141  auc: 0.7366769669598289\n",
            " epoch: 142  auc: 0.7367245067744236\n",
            " epoch: 143  auc: 0.7369146660328024\n",
            " epoch: 144  auc: 0.7369146660328024\n",
            " epoch: 145  auc: 0.7367245067744236\n",
            " epoch: 146  auc: 0.7364868077014499\n",
            " epoch: 147  auc: 0.7362966484430711\n",
            " epoch: 148  auc: 0.7361064891846921\n",
            " epoch: 149  auc: 0.7362491086284764\n",
            " epoch: 150  auc: 0.7362015688138815\n",
            " epoch: 151  auc: 0.7363441882576658\n",
            " epoch: 152  auc: 0.7363441882576659\n",
            " epoch: 153  auc: 0.7364392678868553\n",
            " epoch: 154  auc: 0.7362966484430711\n",
            " epoch: 155  auc: 0.7362491086284764\n",
            " epoch: 156  auc: 0.7361064891846921\n",
            " epoch: 157  auc: 0.7361064891846922\n",
            " epoch: 158  auc: 0.7361540289992868\n",
            " epoch: 159  auc: 0.735963869740908\n",
            " epoch: 160  auc: 0.7360589493700975\n",
            " epoch: 161  auc: 0.7358212502971239\n",
            " epoch: 162  auc: 0.7356548609460423\n",
            " epoch: 163  auc: 0.7355360114095555\n",
            " epoch: 164  auc: 0.7355360114095555\n",
            " epoch: 165  auc: 0.7357737104825292\n",
            " epoch: 166  auc: 0.7358687901117186\n",
            " epoch: 167  auc: 0.736154028999287\n",
            " epoch: 168  auc: 0.7360589493700974\n",
            " epoch: 169  auc: 0.7362015688138817\n",
            " epoch: 170  auc: 0.7363441882576659\n",
            " epoch: 171  auc: 0.7363441882576659\n",
            " epoch: 172  auc: 0.7363441882576658\n",
            " epoch: 173  auc: 0.7362491086284764\n",
            " epoch: 174  auc: 0.7364392678868553\n",
            " epoch: 175  auc: 0.7363441882576658\n",
            " epoch: 176  auc: 0.7363917280722605\n",
            " epoch: 177  auc: 0.7362015688138817\n",
            " epoch: 178  auc: 0.7362015688138817\n",
            " epoch: 179  auc: 0.7357261706679344\n",
            " epoch: 180  auc: 0.7355835512241502\n",
            " epoch: 181  auc: 0.7354884715949607\n",
            " epoch: 182  auc: 0.7354884715949608\n",
            " epoch: 183  auc: 0.735631091038745\n",
            " epoch: 184  auc: 0.7355360114095555\n",
            " epoch: 185  auc: 0.735440931780366\n",
            " epoch: 186  auc: 0.7353933919657712\n",
            " epoch: 187  auc: 0.7355360114095556\n",
            " epoch: 188  auc: 0.7353933919657714\n",
            " epoch: 189  auc: 0.7352983123365818\n",
            " epoch: 190  auc: 0.7351556928927977\n",
            " epoch: 191  auc: 0.7351556928927977\n",
            " epoch: 192  auc: 0.7352983123365818\n",
            " epoch: 193  auc: 0.7354884715949608\n",
            " epoch: 194  auc: 0.7351556928927977\n",
            " epoch: 195  auc: 0.7352032327073924\n",
            " epoch: 196  auc: 0.7349179938198241\n",
            " epoch: 197  auc: 0.7348229141906346\n",
            " epoch: 198  auc: 0.7346802947468506\n",
            " epoch: 199  auc: 0.7344425956738769\n",
            " epoch: 200  auc: 0.7346327549322558\n",
            " epoch: 201  auc: 0.7343950558592821\n",
            " epoch: 202  auc: 0.7343950558592821\n",
            " epoch: 203  auc: 0.7342048966009033\n",
            " epoch: 204  auc: 0.733919657713335\n",
            " epoch: 205  auc: 0.7338245780841455\n",
            " epoch: 206  auc: 0.7334442595673877\n",
            " epoch: 207  auc: 0.7333491799381983\n",
            " epoch: 208  auc: 0.7332541003090088\n",
            " epoch: 209  auc: 0.7329213216068456\n",
            " epoch: 210  auc: 0.7326836225338721\n",
            " epoch: 211  auc: 0.7324934632754933\n",
            " epoch: 212  auc: 0.7322082243879249\n",
            " epoch: 213  auc: 0.7319229855003565\n",
            " epoch: 214  auc: 0.7317803660565724\n",
            " epoch: 215  auc: 0.7315426669835988\n",
            " epoch: 216  auc: 0.7317328262419777\n",
            " epoch: 217  auc: 0.7314951271690041\n",
            " epoch: 218  auc: 0.7315426669835987\n",
            " epoch: 219  auc: 0.7314475873544093\n",
            " epoch: 220  auc: 0.7313525077252199\n",
            " epoch: 221  auc: 0.7313525077252199\n",
            " epoch: 222  auc: 0.7314475873544093\n",
            " epoch: 223  auc: 0.7312098882814357\n",
            " epoch: 224  auc: 0.7311148086522463\n",
            " epoch: 225  auc: 0.7311148086522463\n",
            " epoch: 226  auc: 0.731162348466841\n",
            " epoch: 227  auc: 0.7310672688376516\n",
            " epoch: 228  auc: 0.730639410506299\n",
            " epoch: 229  auc: 0.7307820299500832\n",
            " epoch: 230  auc: 0.7307820299500832\n",
            " epoch: 231  auc: 0.7304492512479202\n",
            " epoch: 232  auc: 0.7303541716187307\n",
            " epoch: 233  auc: 0.7302115521749465\n",
            " epoch: 234  auc: 0.7298312336581887\n",
            " epoch: 235  auc: 0.7297361540289993\n",
            "fold: 2  epoch: 135\n",
            "fold: 2  accuracy: 0.9449685534591195\n",
            "fold: 2  f1_score: 0.05405405405405405\n",
            "fold: 2  auc: 0.737009745661992\n",
            " epoch: 0  auc: 0.36635563849640096\n",
            " epoch: 1  auc: 0.37984537456678225\n",
            " epoch: 2  auc: 0.39408157824580115\n",
            " epoch: 3  auc: 0.40261263663023195\n",
            " epoch: 4  auc: 0.3981338309784057\n",
            " epoch: 5  auc: 0.4101839509464143\n",
            " epoch: 6  auc: 0.4261263663023194\n",
            " epoch: 7  auc: 0.4367368701679552\n",
            " epoch: 8  auc: 0.43966942148760335\n",
            " epoch: 9  auc: 0.4464942681951479\n",
            " epoch: 10  auc: 0.4508664356171687\n",
            " epoch: 11  auc: 0.4535323913623034\n",
            " epoch: 12  auc: 0.45785123966942143\n",
            " epoch: 13  auc: 0.46632897893894965\n",
            " epoch: 14  auc: 0.473900293255132\n",
            " epoch: 15  auc: 0.4844041588909624\n",
            " epoch: 16  auc: 0.49722740602505994\n",
            " epoch: 17  auc: 0.5083444414822714\n",
            " epoch: 18  auc: 0.5196480938416422\n",
            " epoch: 19  auc: 0.5308184484137562\n",
            " epoch: 20  auc: 0.5450279925353239\n",
            " epoch: 21  auc: 0.5556918155158623\n",
            " epoch: 22  auc: 0.5673153825646494\n",
            " epoch: 23  auc: 0.578832311383631\n",
            " epoch: 24  auc: 0.5881098373766995\n",
            " epoch: 25  auc: 0.5998400426552919\n",
            " epoch: 26  auc: 0.6086376966142362\n",
            " epoch: 27  auc: 0.6189816049053586\n",
            " epoch: 28  auc: 0.6290589176219674\n",
            " epoch: 29  auc: 0.639722740602506\n",
            " epoch: 30  auc: 0.6463876299653426\n",
            " epoch: 31  auc: 0.6563049853372434\n",
            " epoch: 32  auc: 0.6644094908024527\n",
            " epoch: 33  auc: 0.6721940815782458\n",
            " epoch: 34  auc: 0.6794988003199147\n",
            " epoch: 35  auc: 0.6870167955211943\n",
            " epoch: 36  auc: 0.6928285790455878\n",
            " epoch: 37  auc: 0.6974673420421221\n",
            " epoch: 38  auc: 0.7010930418555053\n",
            " epoch: 39  auc: 0.7043455078645695\n",
            " epoch: 40  auc: 0.7085043988269795\n",
            " epoch: 41  auc: 0.7125033324446813\n",
            " epoch: 42  auc: 0.7165555851772861\n",
            " epoch: 43  auc: 0.7190615835777127\n",
            " epoch: 44  auc: 0.720554518794988\n",
            " epoch: 45  auc: 0.7223673687016796\n",
            " epoch: 46  auc: 0.7241268994934683\n",
            " epoch: 47  auc: 0.7257264729405491\n",
            " epoch: 48  auc: 0.727645961077046\n",
            " epoch: 49  auc: 0.728552386030392\n",
            " epoch: 50  auc: 0.7291922154092242\n",
            " epoch: 51  auc: 0.7298853639029592\n",
            " epoch: 52  auc: 0.7306318315115968\n",
            " epoch: 53  auc: 0.7318048520394562\n",
            " epoch: 54  auc: 0.7316982138096508\n",
            " epoch: 55  auc: 0.7324446814182884\n",
            " epoch: 56  auc: 0.7330311916822181\n",
            " epoch: 57  auc: 0.7334577446014395\n",
            " epoch: 58  auc: 0.7334577446014396\n",
            " epoch: 59  auc: 0.7339376166355638\n",
            " epoch: 60  auc: 0.7341508930951746\n",
            " epoch: 61  auc: 0.7347907224740069\n",
            " epoch: 62  auc: 0.7345241268994934\n",
            " epoch: 63  auc: 0.7343108504398828\n",
            " epoch: 64  auc: 0.7344708077845907\n",
            " epoch: 65  auc: 0.7344708077845907\n",
            " epoch: 66  auc: 0.7344174886696881\n",
            " epoch: 67  auc: 0.7345774460143961\n",
            " epoch: 68  auc: 0.7343108504398826\n",
            " epoch: 69  auc: 0.7343641695547852\n",
            " epoch: 70  auc: 0.7342575313249798\n",
            " epoch: 71  auc: 0.7345774460143961\n",
            " epoch: 72  auc: 0.7347374033591041\n",
            " epoch: 73  auc: 0.7348973607038123\n",
            " epoch: 74  auc: 0.7356438283124499\n",
            " epoch: 75  auc: 0.7351106371634231\n",
            " epoch: 76  auc: 0.7347374033591042\n",
            " epoch: 77  auc: 0.7348440415889097\n",
            " epoch: 78  auc: 0.735270594508131\n",
            " epoch: 79  auc: 0.7352172753932285\n",
            " epoch: 80  auc: 0.7355905091975472\n",
            " epoch: 81  auc: 0.7352172753932285\n",
            " epoch: 82  auc: 0.7351106371634231\n",
            " epoch: 83  auc: 0.7350573180485204\n",
            " epoch: 84  auc: 0.7348973607038123\n",
            " epoch: 85  auc: 0.7345774460143961\n",
            " epoch: 86  auc: 0.7348973607038123\n",
            " epoch: 87  auc: 0.7352172753932285\n",
            " epoch: 88  auc: 0.735270594508131\n",
            " epoch: 89  auc: 0.7352172753932282\n",
            " epoch: 90  auc: 0.7355905091975472\n",
            " epoch: 91  auc: 0.7357504665422553\n",
            " epoch: 92  auc: 0.7360703812316715\n",
            " epoch: 93  auc: 0.7363902959210877\n",
            " epoch: 94  auc: 0.7367102106105039\n",
            " epoch: 95  auc: 0.7369768061850173\n",
            " epoch: 96  auc: 0.7367102106105039\n",
            " epoch: 97  auc: 0.7370834444148228\n",
            " epoch: 98  auc: 0.7371367635297255\n",
            " epoch: 99  auc: 0.7372967208744334\n",
            " epoch: 100  auc: 0.737190082644628\n",
            " epoch: 101  auc: 0.7369234870701146\n",
            " epoch: 102  auc: 0.736870167955212\n",
            " epoch: 103  auc: 0.7368168488403093\n",
            " epoch: 104  auc: 0.7372967208744334\n",
            " epoch: 105  auc: 0.7380965075979738\n",
            " epoch: 106  auc: 0.7388429752066116\n",
            " epoch: 107  auc: 0.7390029325513195\n",
            " epoch: 108  auc: 0.7387363369768062\n",
            " epoch: 109  auc: 0.7388962943215142\n",
            " epoch: 110  auc: 0.7391628898960277\n",
            " epoch: 111  auc: 0.738949613436417\n",
            " epoch: 112  auc: 0.7392162090109303\n",
            " epoch: 113  auc: 0.7392162090109303\n",
            " epoch: 114  auc: 0.7394294854705412\n",
            " epoch: 115  auc: 0.7392162090109303\n",
            " epoch: 116  auc: 0.7390562516662222\n",
            " epoch: 117  auc: 0.7387363369768062\n",
            " epoch: 118  auc: 0.738949613436417\n",
            " epoch: 119  auc: 0.7387896560917088\n",
            " epoch: 120  auc: 0.7385230605171955\n",
            " epoch: 121  auc: 0.7383631031724873\n",
            " epoch: 122  auc: 0.7385230605171955\n",
            " epoch: 123  auc: 0.7386830178619035\n",
            " epoch: 124  auc: 0.7386830178619035\n",
            " epoch: 125  auc: 0.7386296987470007\n",
            " epoch: 126  auc: 0.7386296987470008\n",
            " epoch: 127  auc: 0.7383631031724873\n",
            " epoch: 128  auc: 0.7376166355638496\n",
            " epoch: 129  auc: 0.7374033591042389\n",
            " epoch: 130  auc: 0.7369234870701147\n",
            " epoch: 131  auc: 0.7366568914956012\n",
            " epoch: 132  auc: 0.7364436150359904\n",
            " epoch: 133  auc: 0.7359104238869635\n",
            " epoch: 134  auc: 0.7358037856571581\n",
            " epoch: 135  auc: 0.7354838709677419\n",
            " epoch: 136  auc: 0.7350573180485204\n",
            " epoch: 137  auc: 0.734950679818715\n",
            " epoch: 138  auc: 0.7350573180485204\n",
            " epoch: 139  auc: 0.7350039989336177\n",
            " epoch: 140  auc: 0.7345241268994934\n",
            " epoch: 141  auc: 0.7347374033591042\n",
            " epoch: 142  auc: 0.7348440415889096\n",
            " epoch: 143  auc: 0.7346307651292988\n",
            " epoch: 144  auc: 0.7344174886696881\n",
            " epoch: 145  auc: 0.734417488669688\n",
            " epoch: 146  auc: 0.734417488669688\n",
            " epoch: 147  auc: 0.7341508930951746\n",
            " epoch: 148  auc: 0.7339909357504665\n",
            " epoch: 149  auc: 0.7338842975206611\n",
            " epoch: 150  auc: 0.7336177019461476\n",
            " epoch: 151  auc: 0.7333511063716341\n",
            " epoch: 152  auc: 0.7331911490269262\n",
            " epoch: 153  auc: 0.7331378299120234\n",
            " epoch: 154  auc: 0.7329245534524126\n",
            " epoch: 155  auc: 0.73287123433751\n",
            " epoch: 156  auc: 0.7329778725673153\n",
            " epoch: 157  auc: 0.7327112769928019\n",
            " epoch: 158  auc: 0.7328712343375099\n",
            " epoch: 159  auc: 0.7324980005331911\n",
            " epoch: 160  auc: 0.7323380431884832\n",
            " epoch: 161  auc: 0.7320181284990668\n",
            " epoch: 162  auc: 0.7317515329245534\n",
            " epoch: 163  auc: 0.73148493735004\n",
            " epoch: 164  auc: 0.7312183417755265\n",
            " epoch: 165  auc: 0.7311117035457211\n",
            " epoch: 166  auc: 0.7312716608904292\n",
            " epoch: 167  auc: 0.7306851506264995\n",
            " epoch: 168  auc: 0.7303652359370834\n",
            " epoch: 169  auc: 0.7303652359370835\n",
            " epoch: 170  auc: 0.7303119168221807\n",
            " epoch: 171  auc: 0.7301519594774727\n",
            " epoch: 172  auc: 0.7303119168221809\n",
            " epoch: 173  auc: 0.7300453212476673\n",
            " epoch: 174  auc: 0.7298853639029591\n",
            " epoch: 175  auc: 0.7299386830178619\n",
            " epoch: 176  auc: 0.7298320447880565\n",
            " epoch: 177  auc: 0.7292988536390296\n",
            " epoch: 178  auc: 0.7290855771794189\n",
            " epoch: 179  auc: 0.728872300719808\n",
            " epoch: 180  auc: 0.7285523860303919\n",
            " epoch: 181  auc: 0.7280725139962676\n",
            " epoch: 182  auc: 0.7275393228472408\n",
            " epoch: 183  auc: 0.7273793655025326\n",
            " epoch: 184  auc: 0.7271127699280192\n",
            " epoch: 185  auc: 0.7268461743535056\n",
            " epoch: 186  auc: 0.726632897893895\n",
            " epoch: 187  auc: 0.7264729405491869\n",
            " epoch: 188  auc: 0.7262063449746734\n",
            " epoch: 189  auc: 0.7261530258597707\n",
            " epoch: 190  auc: 0.7259930685150626\n",
            " epoch: 191  auc: 0.7259930685150626\n",
            " epoch: 192  auc: 0.725779792055452\n",
            " epoch: 193  auc: 0.7256198347107439\n",
            " epoch: 194  auc: 0.7256198347107439\n",
            " epoch: 195  auc: 0.7255665155958412\n",
            " epoch: 196  auc: 0.7252999200213277\n",
            " epoch: 197  auc: 0.7251932817915222\n",
            " epoch: 198  auc: 0.724873367102106\n",
            " epoch: 199  auc: 0.7250333244468141\n",
            " epoch: 200  auc: 0.7248733671021061\n",
            " epoch: 201  auc: 0.7246067715275927\n",
            " epoch: 202  auc: 0.7244468141828845\n",
            " epoch: 203  auc: 0.7240735803785657\n",
            " epoch: 204  auc: 0.7239136230338576\n",
            " epoch: 205  auc: 0.7237003465742469\n",
            " epoch: 206  auc: 0.7237003465742469\n",
            " epoch: 207  auc: 0.7235937083444415\n",
            " epoch: 208  auc: 0.7233804318848307\n",
            " epoch: 209  auc: 0.7234337509997334\n",
            " epoch: 210  auc: 0.723327112769928\n",
            " epoch: 211  auc: 0.7231671554252199\n",
            " epoch: 212  auc: 0.7233804318848307\n",
            " epoch: 213  auc: 0.7231671554252199\n",
            " epoch: 214  auc: 0.7231138363103172\n",
            "fold: 3  epoch: 114\n",
            "fold: 3  accuracy: 0.9512578616352201\n",
            "fold: 3  f1_score: 0.0\n",
            "fold: 3  auc: 0.7394294854705412\n",
            " epoch: 0  auc: 0.4455536912751678\n",
            " epoch: 1  auc: 0.4336828859060402\n",
            " epoch: 2  auc: 0.43162751677852346\n",
            " epoch: 3  auc: 0.4356963087248322\n",
            " epoch: 4  auc: 0.43640939597315437\n",
            " epoch: 5  auc: 0.44211409395973156\n",
            " epoch: 6  auc: 0.44421140939597314\n",
            " epoch: 7  auc: 0.44702181208053693\n",
            " epoch: 8  auc: 0.451258389261745\n",
            " epoch: 9  auc: 0.4523070469798658\n",
            " epoch: 10  auc: 0.45281040268456374\n",
            " epoch: 11  auc: 0.4518875838926175\n",
            " epoch: 12  auc: 0.45385906040268453\n",
            " epoch: 13  auc: 0.46153523489932885\n",
            " epoch: 14  auc: 0.46535234899328853\n",
            " epoch: 15  auc: 0.46719798657718126\n",
            " epoch: 16  auc: 0.47093120805369126\n",
            " epoch: 17  auc: 0.4745385906040269\n",
            " epoch: 18  auc: 0.4802013422818792\n",
            " epoch: 19  auc: 0.48640939597315436\n",
            " epoch: 20  auc: 0.49223993288590606\n",
            " epoch: 21  auc: 0.49899328859060404\n",
            " epoch: 22  auc: 0.5105704697986577\n",
            " epoch: 23  auc: 0.520008389261745\n",
            " epoch: 24  auc: 0.5298657718120805\n",
            " epoch: 25  auc: 0.5411073825503356\n",
            " epoch: 26  auc: 0.5552013422818792\n",
            " epoch: 27  auc: 0.567743288590604\n",
            " epoch: 28  auc: 0.5815855704697986\n",
            " epoch: 29  auc: 0.5933305369127517\n",
            " epoch: 30  auc: 0.6046979865771811\n",
            " epoch: 31  auc: 0.6156669463087249\n",
            " epoch: 32  auc: 0.6264261744966443\n",
            " epoch: 33  auc: 0.6371224832214765\n",
            " epoch: 34  auc: 0.6472315436241611\n",
            " epoch: 35  auc: 0.6545302013422819\n",
            " epoch: 36  auc: 0.6614093959731544\n",
            " epoch: 37  auc: 0.6702600671140939\n",
            " epoch: 38  auc: 0.675755033557047\n",
            " epoch: 39  auc: 0.6805788590604027\n",
            " epoch: 40  auc: 0.685989932885906\n",
            " epoch: 41  auc: 0.6901426174496645\n",
            " epoch: 42  auc: 0.6933724832214765\n",
            " epoch: 43  auc: 0.6971895973154362\n",
            " epoch: 44  auc: 0.7001258389261744\n",
            " epoch: 45  auc: 0.702265100671141\n",
            " epoch: 46  auc: 0.703481543624161\n",
            " epoch: 47  auc: 0.7044463087248322\n",
            " epoch: 48  auc: 0.7056627516778523\n",
            " epoch: 49  auc: 0.7067114093959732\n",
            " epoch: 50  auc: 0.7071728187919464\n",
            " epoch: 51  auc: 0.7071308724832215\n",
            " epoch: 52  auc: 0.708263422818792\n",
            " epoch: 53  auc: 0.7093540268456376\n",
            " epoch: 54  auc: 0.7097315436241611\n",
            " epoch: 55  auc: 0.7102348993288591\n",
            " epoch: 56  auc: 0.7103607382550335\n",
            " epoch: 57  auc: 0.7105285234899329\n",
            " epoch: 58  auc: 0.7105704697986577\n",
            " epoch: 59  auc: 0.7109060402684564\n",
            " epoch: 60  auc: 0.7111157718120805\n",
            " epoch: 61  auc: 0.7110318791946308\n",
            " epoch: 62  auc: 0.7111577181208054\n",
            " epoch: 63  auc: 0.7113255033557048\n",
            " epoch: 64  auc: 0.7111157718120805\n",
            " epoch: 65  auc: 0.7116610738255034\n",
            " epoch: 66  auc: 0.7119546979865772\n",
            " epoch: 67  auc: 0.7124580536912752\n",
            " epoch: 68  auc: 0.7129194630872483\n",
            " epoch: 69  auc: 0.7131711409395973\n",
            " epoch: 70  auc: 0.7131711409395973\n",
            " epoch: 71  auc: 0.7135486577181208\n",
            " epoch: 72  auc: 0.7139681208053692\n",
            " epoch: 73  auc: 0.7140520134228188\n",
            " epoch: 74  auc: 0.7145553691275167\n",
            " epoch: 75  auc: 0.7147231543624161\n",
            " epoch: 76  auc: 0.7150587248322148\n",
            " epoch: 77  auc: 0.7153104026845638\n",
            " epoch: 78  auc: 0.7156879194630873\n",
            " epoch: 79  auc: 0.7160234899328859\n",
            " epoch: 80  auc: 0.7164848993288591\n",
            " epoch: 81  auc: 0.7167365771812081\n",
            " epoch: 82  auc: 0.7171979865771813\n",
            " epoch: 83  auc: 0.7178271812080537\n",
            " epoch: 84  auc: 0.718498322147651\n",
            " epoch: 85  auc: 0.7190436241610738\n",
            " epoch: 86  auc: 0.7195889261744967\n",
            " epoch: 87  auc: 0.7200503355704698\n",
            " epoch: 88  auc: 0.7207214765100671\n",
            " epoch: 89  auc: 0.7210570469798658\n",
            " epoch: 90  auc: 0.721728187919463\n",
            " epoch: 91  auc: 0.722273489932886\n",
            " epoch: 92  auc: 0.7227768456375839\n",
            " epoch: 93  auc: 0.7236996644295302\n",
            " epoch: 94  auc: 0.7242869127516778\n",
            " epoch: 95  auc: 0.7246644295302013\n",
            " epoch: 96  auc: 0.725251677852349\n",
            " epoch: 97  auc: 0.7255872483221477\n",
            " epoch: 98  auc: 0.7256291946308724\n",
            " epoch: 99  auc: 0.7261744966442953\n",
            " epoch: 100  auc: 0.7265520134228188\n",
            " epoch: 101  auc: 0.7268875838926174\n",
            " epoch: 102  auc: 0.7273909395973155\n",
            " epoch: 103  auc: 0.727474832214765\n",
            " epoch: 104  auc: 0.72751677852349\n",
            " epoch: 105  auc: 0.7278104026845638\n",
            " epoch: 106  auc: 0.7278104026845638\n",
            " epoch: 107  auc: 0.7279781879194631\n",
            " epoch: 108  auc: 0.7283137583892617\n",
            " epoch: 109  auc: 0.7280201342281879\n",
            " epoch: 110  auc: 0.7279362416107382\n",
            " epoch: 111  auc: 0.7279781879194631\n",
            " epoch: 112  auc: 0.7283557046979866\n",
            " epoch: 113  auc: 0.7287332214765101\n",
            " epoch: 114  auc: 0.7289010067114093\n",
            " epoch: 115  auc: 0.7290687919463087\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d777649ba562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mtrainpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_bf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtestpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d777649ba562>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_bin, t)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x_bin)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmha_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmha_att_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultihead_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmha_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmha_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmha_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/layer_normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0;31m# Calculate the moments on the last axis (layer activations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mmoments_v2\u001b[0;34m(x, axes, shift, keepdims, name)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[0mTwo\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m   \"\"\"\n\u001b[0;32m-> 1408\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mmoments\u001b[0;34m(x, axes, shift, name, keep_dims, keepdims)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;31m# Compute true mean while keeping the dims for proper broadcasting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;31m# sample variance, not unbiased variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;31m# Note: stop_gradient does not change the gradient that gets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2621\u001b[0m       gen_math_ops.mean(\n\u001b[1;32m   2622\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   6277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6278\u001b[0m       return mean_eager_fallback(\n\u001b[0;32m-> 6279\u001b[0;31m           input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   6280\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6281\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean_eager_fallback\u001b[0;34m(input, axis, keep_dims, name, ctx)\u001b[0m\n\u001b[1;32m   6309\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"keep_dims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6310\u001b[0m   _result = _execute.execute(b\"Mean\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 6311\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   6312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6313\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from loading_data import *\n",
        "from model import *\n",
        "from config import config\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    np.random.seed(2019)\n",
        "    tf.random.set_seed(2019)\n",
        "\n",
        "    config = config()\n",
        "    data_file = './sample_data.xlsx'\n",
        "\n",
        "    if len(sys.argv) < 2:\n",
        "        data_file = sys.argv[1]\n",
        "\n",
        "    x_bin_features, feats, tokens, feat_max, y = data_preparation(data_file)\n",
        "\n",
        "    kf = KFold(n_splits=5, random_state=2019, shuffle=True)\n",
        "    fold = 1\n",
        "\n",
        "    accuracy = []\n",
        "    f1 = []\n",
        "    auc = []\n",
        "\n",
        "    for train_index, test_index in kf.split(x_bin_features):\n",
        "\n",
        "        x_bf_train, x_bf_test = x_bin_features[train_index], x_bin_features[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "\n",
        "        def compute_loss(label, pred):\n",
        "\n",
        "            return criterion(label, pred)\n",
        "\n",
        "\n",
        "        def train_step(x_bin, t):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred, _, _ = model(x_bin)\n",
        "                loss = compute_loss(t, pred)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            train_loss(loss)\n",
        "\n",
        "            return pred\n",
        "\n",
        "\n",
        "        def test_step(x_bin, t):\n",
        "\n",
        "            pred, _, _ = model(x_bin)\n",
        "            loss = compute_loss(t, pred)\n",
        "            test_loss(loss)\n",
        "\n",
        "            return pred\n",
        "\n",
        "\n",
        "        model = Graph(tokens, config.embedding, feat_max, config.num_heads, config.dropout_rate)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate, beta_1=config.beta_1,\n",
        "                                             beta_2=config.beta_2, epsilon=config.epsilon)\n",
        "\n",
        "        epochs = config.epochs\n",
        "        batch_size = config.batch_size\n",
        "        n_batches = x_bin_features.shape[0] // batch_size\n",
        "\n",
        "        criterion = tf.losses.BinaryCrossentropy()\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean()\n",
        "        test_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "        es = []\n",
        "        preds_temp = []\n",
        "        stop = False\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # early stopping\n",
        "            if stop == False:\n",
        "                _x_bf_train, _y_train = shuffle(x_bf_train, y_train, random_state=2019)\n",
        "\n",
        "                for batch in range(n_batches):\n",
        "                    start = batch * batch_size\n",
        "                    end = start + batch_size\n",
        "                    trainpreds = train_step(_x_bf_train[start:end], _y_train[start:end])\n",
        "\n",
        "                testpreds = test_step(x_bf_test, y_test)\n",
        "                score = roc_auc_score(y_test, testpreds)\n",
        "                es.append(score)\n",
        "\n",
        "                print(' epoch:', epoch, ' auc:', score)\n",
        "                preds_temp.append(testpreds)\n",
        "\n",
        "                if len(es) - np.argmax(es) > config.tolerance:\n",
        "                    stop = True\n",
        "\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        num = np.argmax(es)\n",
        "        print('fold:', fold, ' epoch:', num)\n",
        "\n",
        "        pred_temp_thres = np.int32(preds_temp[num] > 0.5)\n",
        "\n",
        "        acc_temp = accuracy_score(y_test, pred_temp_thres)\n",
        "        accuracy.append(acc_temp)\n",
        "        print('fold:', fold, ' accuracy:', acc_temp)\n",
        "\n",
        "        f1_temp = f1_score(y_test, pred_temp_thres)\n",
        "        f1.append(f1_temp)\n",
        "        print('fold:', fold, ' f1_score:', f1_temp)\n",
        "\n",
        "        auc_temp = roc_auc_score(y_test, preds_temp[num])\n",
        "        auc.append(auc_temp)\n",
        "        print('fold:', fold, ' auc:', auc_temp)\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "    print('###################################################')\n",
        "    print('auc:', np.mean(auc))\n",
        "    print('f1 score:', np.mean(f1))\n",
        "    print('accuracy:', np.mean(accuracy))\n",
        "    print('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "medical_diagnosis.ipynb",
      "provenance": [],
      "mount_file_id": "1f2GwhYJ9vAZYuclJ6JEudSXbiwHuKrZ8",
      "authorship_tag": "ABX9TyOEKAB9wuyPS8DzDZZvzU3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}