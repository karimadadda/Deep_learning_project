{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMI-NET",
      "provenance": [],
      "authorship_tag": "ABX9TyPhThOd2gvzJ/W99otUowJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimadadda/Deep_learning_project/blob/main/AMI_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des biblioth√©ques"
      ],
      "metadata": {
        "id": "iWEbcxVjBxGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gdYTE_PNBkyE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data and preprocess\n"
      ],
      "metadata": {
        "id": "MC0ELGEoCtBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preparation(path, type='excel'):\n",
        "\n",
        "    if type is 'csv':\n",
        "        df = pd.read_csv(path)\n",
        "    if type is 'excel':\n",
        "        df = pd.read_excel(path)\n",
        "\n",
        "    # x and y\n",
        "    y = np.array(df['y'])\n",
        "    x = df.drop(['y'], axis=1)\n",
        "\n",
        "    # for each patient, convert their containing symptoms to a list of words\n",
        "    bin_feats = []\n",
        "\n",
        "    for i in x.columns:\n",
        "        if len(x[i].unique()) == 2:\n",
        "            bin_feats.append(i)\n",
        "\n",
        "    x_bin = x[bin_feats]\n",
        "    x_bin = x_bin.replace(0, np.nan)\n",
        "    x_bin_features = bin_gen(x_bin)\n",
        "\n",
        "    # pad to the maximum length and convert to matrix\n",
        "    x_bin_features, feats, tokens, feat_max = bin_pad_convert(x_bin_features, bin_feats)\n",
        "\n",
        "    return x_bin_features, feats, tokens, feat_max, y\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "05RZd1_gCo9T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to a list of words for binary features\n",
        "def bin_gen(x):\n",
        "\n",
        "    x_features = []\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        index = x.columns[x.iloc[i, :].notnull()]\n",
        "        feats = np.array(index)\n",
        "        x_features.append(feats)\n",
        "\n",
        "    return np.array(x_features)"
      ],
      "metadata": {
        "id": "RPkWpF9rGp7M"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to matrix and generate descriptions (binary features)\n",
        "def bin_pad_convert(txt_features, bin_feat_list):\n",
        "\n",
        "    bin_feat_max = max([len(feat) for feat in txt_features])\n",
        "    bin_feats = ['pad'] + bin_feat_list\n",
        "    tokens = len(bin_feats)\n",
        "\n",
        "    x_features = np.zeros((len(txt_features), bin_feat_max), dtype='int32')\n",
        "    feat_index = dict([(char, i) for i, char in enumerate(bin_feats)])\n",
        "\n",
        "    for i, input_text in enumerate(txt_features):\n",
        "        for t, char in enumerate(input_text):\n",
        "            x_features[i, t] = feat_index[char]\n",
        "\n",
        "    return x_features, bin_feats, tokens, bin_feat_max"
      ],
      "metadata": {
        "id": "E48k7zYtG3dz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_preparation('/content/sample_data.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6rGNqvRD7w0",
        "outputId": "dc611c4f-4d68-4a2f-99fe-ed7764d48264"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 6,  8, 10, ...,  0,  0,  0],\n",
              "        [ 3,  5,  8, ...,  0,  0,  0],\n",
              "        [ 4,  8, 10, ...,  0,  0,  0],\n",
              "        ...,\n",
              "        [ 6, 10, 27, ...,  0,  0,  0],\n",
              "        [ 4, 10, 28, ...,  0,  0,  0],\n",
              "        [ 4,  7,  8, ...,  0,  0,  0]], dtype=int32),\n",
              " ['pad',\n",
              "  'income_0',\n",
              "  'income_1',\n",
              "  'marriage_0',\n",
              "  'income_2',\n",
              "  'income_3',\n",
              "  'income_4',\n",
              "  'marriage_1',\n",
              "  'job_0',\n",
              "  'marriage_2',\n",
              "  'marriage_3',\n",
              "  'HDL_0',\n",
              "  'HDL_1',\n",
              "  'prolactin_0',\n",
              "  'prolactin_1',\n",
              "  'prolactin_2',\n",
              "  'glucose_0',\n",
              "  'glucose_1',\n",
              "  'glucose_2',\n",
              "  'triglyceride_0',\n",
              "  'triglyceride_1',\n",
              "  'triglyceride_2',\n",
              "  'hemameba_0',\n",
              "  'hemameba_1',\n",
              "  'ACTH_0',\n",
              "  'ACTH_1',\n",
              "  'ACTH_2',\n",
              "  'length_0',\n",
              "  'length_1',\n",
              "  'length_2',\n",
              "  'LOS_0',\n",
              "  'LOS_1',\n",
              "  'LOS_2',\n",
              "  'MECT_0',\n",
              "  'MECT_1',\n",
              "  'MECT_2',\n",
              "  'olan_0',\n",
              "  'olan_1',\n",
              "  'olan_2',\n",
              "  'CDT_0',\n",
              "  'CDT_1',\n",
              "  'CDT_2',\n",
              "  'AO_0',\n",
              "  'AO_1',\n",
              "  'AO_2',\n",
              "  'Risperidone_0',\n",
              "  'Risperidone_1',\n",
              "  'Risperidone_2',\n",
              "  'gap_0',\n",
              "  'gap_1',\n",
              "  'gap_2',\n",
              "  'total_length_0',\n",
              "  'total_length_1',\n",
              "  'total_length_2',\n",
              "  'lorazepam_0',\n",
              "  'lorazepam_1',\n",
              "  'lorazepam_2',\n",
              "  'Amisulpride_0',\n",
              "  'Amisulpride_1',\n",
              "  'Amisulpride_2',\n",
              "  'Haloperidol_0',\n",
              "  'Haloperidol_1',\n",
              "  'Haloperidol_2',\n",
              "  'Aripiprazole_0',\n",
              "  'Aripiprazole_1',\n",
              "  'Aripiprazole_2',\n",
              "  'TAG_0',\n",
              "  'TAG_1',\n",
              "  'TAG_2',\n",
              "  'PET_0',\n",
              "  'PET_1',\n",
              "  'PET_2',\n",
              "  'Perphenazine_0',\n",
              "  'Perphenazine_1',\n",
              "  'Perphenazine_2',\n",
              "  'SEROQUEL_0',\n",
              "  'SEROQUEL_1',\n",
              "  'SEROQUEL_2',\n",
              "  'Ziprasidone_0',\n",
              "  'Ziprasidone_1',\n",
              "  'Ziprasidone_2',\n",
              "  'Penfluridol_0',\n",
              "  'Penfluridol_1',\n",
              "  'Penfluridol_2',\n",
              "  'RisperdalConsta_0',\n",
              "  'RisperdalConsta_1',\n",
              "  'Sulpiride_0',\n",
              "  'Sulpiride_1',\n",
              "  'Sulpiride_2'],\n",
              " 89,\n",
              " 21,\n",
              " array([0, 0, 0, ..., 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## gated attention based multi-instance pooling layer\n",
        "class MIL_gated_attention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model):\n",
        "\n",
        "        super(MIL_gated_attention, self).__init__()\n",
        "\n",
        "        self.w1 = tf.keras.layers.Dense(d_model)\n",
        "        self.w2 = tf.keras.layers.Dense(d_model)\n",
        "        self.w3 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        # linear projection\n",
        "        alpha = tf.tanh(self.w1(x))\n",
        "\n",
        "        # gate mechanism\n",
        "        gate = tf.nn.sigmoid(self.w2(x))\n",
        "        alpha = self.w3(tf.multiply(alpha, gate))\n",
        "\n",
        "        # attention weights\n",
        "        attention_weights = tf.nn.softmax(alpha)\n",
        "\n",
        "        # output\n",
        "        output = tf.multiply(x, attention_weights)\n",
        "        output = tf.reduce_mean(output, axis=-1)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "wGVdDQuaHxF4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# multi-head attention layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # scaled dot product attention\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        # scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.sqrt(dk)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def call(self, v, k, q):\n",
        "\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "fETfdgWdD8eg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AMI-NET\n",
        "class Graph(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, tokens, d_model, feat_max, num_heads, rate):\n",
        "\n",
        "        super(Graph, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(tokens, d_model)\n",
        "        self.multihead_att = MultiHeadAttention(d_model, num_heads)\n",
        "        self.pooling = MIL_gated_attention(feat_max)\n",
        "        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.w1 = tf.keras.layers.Dense(d_model/2, activation='relu')\n",
        "        self.w2 = tf.keras.layers.Dense(d_model/4)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x_bin):\n",
        "\n",
        "        # word embedding\n",
        "        x = self.embedding(x_bin)\n",
        "\n",
        "        # multi-head attention\n",
        "        mha_out, mha_att_matrix = self.multihead_att(x, x, x)\n",
        "        mha_out = self.dropout1(mha_out)\n",
        "        out = self.ln(x + mha_out)\n",
        "\n",
        "        # fully connected layers\n",
        "        x_dense1 = self.w1(out)\n",
        "        x_dense2 = self.w2(x_dense1)\n",
        "        x_dense2_drop = self.dropout2(x_dense2)\n",
        "\n",
        "        # Instance-level Pooling\n",
        "        rep = tf.reduce_sum(x_dense2_drop, axis=-1)\n",
        "\n",
        "        # Bag-level Pooling\n",
        "        mil_out, mil_att_matrix = self.pooling(rep)\n",
        "        pred = tf.nn.sigmoid(mil_out)\n",
        "\n",
        "        return pred, mha_att_matrix, mil_att_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "g3yeMnXmHH8b"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config(object):\n",
        "\n",
        "    # training\n",
        "    epochs = 500\n",
        "    batch_size = 64\n",
        "\n",
        "    # adam\n",
        "    learning_rate = 0.00001\n",
        "    beta_1 = 0.9\n",
        "    beta_2 = 0.98\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # embedding\n",
        "    embedding = 128\n",
        "\n",
        "    # multi-head attention\n",
        "    num_heads = 4\n",
        "\n",
        "    # dropout\n",
        "    dropout_rate = 0.3\n",
        "\n",
        "    # early stopping tolerance epochs\n",
        "    tolerance = 100"
      ],
      "metadata": {
        "id": "8r33Tfl0Ipo7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    np.random.seed(2019)\n",
        "    tf.random.set_seed(2019)\n",
        "    config = config()\n",
        "\n",
        "\n",
        "    data_file = '/content/sample_data.xlsx'\n",
        "\n",
        "    if len(sys.argv) > 2:\n",
        "        data_file = sys.argv[1]\n",
        "\n",
        "    x_bin_features, feats, tokens, feat_max, y = data_preparation('/content/sample_data.xlsx')\n",
        "\n",
        "    kf = KFold(n_splits=5, random_state=2019, shuffle=True)\n",
        "    fold = 1\n",
        "\n",
        "    accuracy = []\n",
        "    f1 = []\n",
        "    auc = []\n",
        "\n",
        "    for train_index, test_index in kf.split(x_bin_features):\n",
        "\n",
        "        x_bf_train, x_bf_test = x_bin_features[train_index], x_bin_features[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "\n",
        "        def compute_loss(label, pred):\n",
        "\n",
        "            return criterion(label, pred)\n",
        "\n",
        "\n",
        "        def train_step(x_bin, t):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred, _, _ = model(x_bin)\n",
        "                loss = compute_loss(t, pred)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            train_loss(loss)\n",
        "\n",
        "            return pred\n",
        "\n",
        "\n",
        "        def test_step(x_bin, t):\n",
        "\n",
        "            pred, _, _ = model(x_bin)\n",
        "            loss = compute_loss(t, pred)\n",
        "            test_loss(loss)\n",
        "\n",
        "            return pred\n",
        "\n",
        "\n",
        "        model = Graph(tokens, config.embedding, feat_max, config.num_heads, config.dropout_rate)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate, beta_1=config.beta_1,\n",
        "                                             beta_2=config.beta_2, epsilon=config.epsilon)\n",
        "\n",
        "        epochs = config.epochs\n",
        "        batch_size = config.batch_size\n",
        "        n_batches = x_bin_features.shape[0] // batch_size\n",
        "\n",
        "        criterion = tf.losses.BinaryCrossentropy()\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean()\n",
        "        test_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "        es = []\n",
        "        preds_temp = []\n",
        "        stop = False\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # early stopping\n",
        "            if stop == False:\n",
        "                _x_bf_train, _y_train = shuffle(x_bf_train, y_train, random_state=2019)\n",
        "\n",
        "                for batch in range(n_batches):\n",
        "                    start = batch * batch_size\n",
        "                    end = start + batch_size\n",
        "                    trainpreds = train_step(_x_bf_train[start:end], _y_train[start:end])\n",
        "\n",
        "                testpreds = test_step(x_bf_test, y_test)\n",
        "                score = roc_auc_score(y_test, testpreds)\n",
        "                es.append(score)\n",
        "\n",
        "                print(' epoch:', epoch, ' auc:', score)\n",
        "                preds_temp.append(testpreds)\n",
        "\n",
        "                if len(es) - np.argmax(es) > config.tolerance:\n",
        "                    stop = True\n",
        "\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        num = np.argmax(es)\n",
        "        print('fold:', fold, ' epoch:', num)\n",
        "\n",
        "        pred_temp_thres = np.int32(preds_temp[num] > 0.5)\n",
        "\n",
        "        acc_temp = accuracy_score(y_test, pred_temp_thres)\n",
        "        accuracy.append(acc_temp)\n",
        "        print('fold:', fold, ' accuracy:', acc_temp)\n",
        "\n",
        "        f1_temp = f1_score(y_test, pred_temp_thres)\n",
        "        f1.append(f1_temp)\n",
        "        print('fold:', fold, ' f1_score:', f1_temp)\n",
        "\n",
        "        auc_temp = roc_auc_score(y_test, preds_temp[num])\n",
        "        auc.append(auc_temp)\n",
        "        print('fold:', fold, ' auc:', auc_temp)\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "    print('###################################################')\n",
        "    print('auc:', np.mean(auc))\n",
        "    print('f1 score:', np.mean(f1))\n",
        "    print('accuracy:', np.mean(accuracy))\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPj-zI3jJLZx",
        "outputId": "6149f1b3-0700-4abb-9bd6-4810aa3858f5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " epoch: 0  auc: 0.40541252965468766\n",
            " epoch: 1  auc: 0.41477023108689914\n",
            " epoch: 2  auc: 0.41696687461558735\n",
            " epoch: 3  auc: 0.42263421491960285\n",
            " epoch: 4  auc: 0.42636850891837275\n",
            " epoch: 5  auc: 0.4330463052455848\n",
            " epoch: 6  auc: 0.43950443721992793\n",
            " epoch: 7  auc: 0.44367805992443543\n",
            " epoch: 8  auc: 0.4464897636411563\n",
            " epoch: 9  auc: 0.4500043932870574\n",
            " epoch: 10  auc: 0.45184957385115543\n",
            " epoch: 11  auc: 0.45523240488533523\n",
            " epoch: 12  auc: 0.4622177313065635\n",
            " epoch: 13  auc: 0.4718390299622177\n",
            " epoch: 14  auc: 0.48110886565328176\n",
            " epoch: 15  auc: 0.4939372638608207\n",
            " epoch: 16  auc: 0.5080836481855724\n",
            " epoch: 17  auc: 0.5228450926983569\n",
            " epoch: 18  auc: 0.5406379052807312\n",
            " epoch: 19  auc: 0.5587382479571216\n",
            " epoch: 20  auc: 0.5773218522098235\n",
            " epoch: 21  auc: 0.5977506370266233\n",
            " epoch: 22  auc: 0.6188384149020296\n",
            " epoch: 23  auc: 0.6395307969422722\n",
            " epoch: 24  auc: 0.6610579035234162\n",
            " epoch: 25  auc: 0.6806519637993147\n",
            " epoch: 26  auc: 0.6978297161936561\n",
            " epoch: 27  auc: 0.7136894824707847\n",
            " epoch: 28  auc: 0.7283630612424216\n",
            " epoch: 29  auc: 0.7412793251911081\n",
            " epoch: 30  auc: 0.7539319919163519\n",
            " epoch: 31  auc: 0.7657499341006941\n",
            " epoch: 32  auc: 0.7756567964150778\n",
            " epoch: 33  auc: 0.7825762235304454\n",
            " epoch: 34  auc: 0.7878921008698708\n",
            " epoch: 35  auc: 0.7908356031983129\n",
            " epoch: 36  auc: 0.7937791055267551\n",
            " epoch: 37  auc: 0.7929883138564273\n",
            " epoch: 38  auc: 0.7948993937263861\n",
            " epoch: 39  auc: 0.7941745013619189\n",
            " epoch: 40  auc: 0.7936912397856076\n",
            " epoch: 41  auc: 0.791582461998067\n",
            " epoch: 42  auc: 0.7908575696335999\n",
            " epoch: 43  auc: 0.7907916703277392\n",
            " epoch: 44  auc: 0.7895176170811\n",
            " epoch: 45  auc: 0.7871013091995431\n",
            " epoch: 46  auc: 0.7859590545646253\n",
            " epoch: 47  auc: 0.7853439943765925\n",
            " epoch: 48  auc: 0.784333538353396\n",
            " epoch: 49  auc: 0.7825762235304455\n",
            " epoch: 50  auc: 0.782092961954134\n",
            " epoch: 51  auc: 0.7815218346366751\n",
            " epoch: 52  auc: 0.780423512872331\n",
            " epoch: 53  auc: 0.7797645198137246\n",
            " epoch: 54  auc: 0.7791055267551181\n",
            " epoch: 55  auc: 0.7778534399437659\n",
            " epoch: 56  auc: 0.7761180915561023\n",
            " epoch: 57  auc: 0.7748440383094631\n",
            " epoch: 58  auc: 0.7742729109920042\n",
            " epoch: 59  auc: 0.7729109920042175\n",
            " epoch: 60  auc: 0.7716808716281522\n",
            " epoch: 61  auc: 0.7715490730164308\n",
            " epoch: 62  auc: 0.7710658114401195\n",
            " epoch: 63  auc: 0.7704068183815131\n",
            " epoch: 64  auc: 0.7700114225463491\n",
            " epoch: 65  auc: 0.769308496617169\n",
            " epoch: 66  auc: 0.7686934364291362\n",
            " epoch: 67  auc: 0.7684298392056937\n",
            " epoch: 68  auc: 0.7674633160530709\n",
            " epoch: 69  auc: 0.7672436517002021\n",
            " epoch: 70  auc: 0.7667603901238907\n",
            " epoch: 71  auc: 0.7665846586415956\n",
            " epoch: 72  auc: 0.7662771285475792\n",
            " epoch: 73  auc: 0.7660135313241367\n",
            " epoch: 74  auc: 0.765222739653809\n",
            " epoch: 75  auc: 0.7648712766892188\n",
            " epoch: 76  auc: 0.764124417889465\n",
            " epoch: 77  auc: 0.7638608206660223\n",
            " epoch: 78  auc: 0.7636850891837271\n",
            " epoch: 79  auc: 0.7636850891837274\n",
            " epoch: 80  auc: 0.7640365521483173\n",
            " epoch: 81  auc: 0.7636411563131534\n",
            " epoch: 82  auc: 0.7635093577014321\n",
            " epoch: 83  auc: 0.7634654248308583\n",
            " epoch: 84  auc: 0.7636850891837272\n",
            " epoch: 85  auc: 0.7633775590897108\n",
            " epoch: 86  auc: 0.7630260961251208\n",
            " epoch: 87  auc: 0.7630260961251207\n",
            " epoch: 88  auc: 0.7629821632545472\n",
            " epoch: 89  auc: 0.7621913715842193\n",
            " epoch: 90  auc: 0.7621913715842193\n",
            " epoch: 91  auc: 0.7618399086196291\n",
            " epoch: 92  auc: 0.7619277743607767\n",
            " epoch: 93  auc: 0.7621035058430718\n",
            " epoch: 94  auc: 0.7623231701959406\n",
            " epoch: 95  auc: 0.7621474387136455\n",
            " epoch: 96  auc: 0.7617959757490554\n",
            " epoch: 97  auc: 0.7617081100079078\n",
            " epoch: 98  auc: 0.7618399086196291\n",
            " epoch: 99  auc: 0.7619717072313504\n",
            " epoch: 100  auc: 0.7623231701959406\n",
            " epoch: 101  auc: 0.7623231701959406\n",
            " epoch: 102  auc: 0.7623671030665143\n",
            " epoch: 103  auc: 0.7624989016782356\n",
            " epoch: 104  auc: 0.7624549688076618\n",
            " epoch: 105  auc: 0.7626746331605306\n",
            " epoch: 106  auc: 0.7625867674193832\n",
            " epoch: 107  auc: 0.7624989016782356\n",
            " epoch: 108  auc: 0.7630260961251208\n",
            " epoch: 109  auc: 0.7632896933485633\n",
            " epoch: 110  auc: 0.762982163254547\n",
            " epoch: 111  auc: 0.7632018276074158\n",
            " epoch: 112  auc: 0.7635093577014321\n",
            " epoch: 113  auc: 0.7634214919602846\n",
            " epoch: 114  auc: 0.7630700289956946\n",
            " epoch: 115  auc: 0.7635093577014321\n",
            " epoch: 116  auc: 0.7634654248308583\n",
            " epoch: 117  auc: 0.7632018276074158\n",
            " epoch: 118  auc: 0.7632457604779896\n",
            " epoch: 119  auc: 0.7635093577014322\n",
            " epoch: 120  auc: 0.7634214919602846\n",
            " epoch: 121  auc: 0.7632896933485634\n",
            " epoch: 122  auc: 0.7632018276074158\n",
            " epoch: 123  auc: 0.7628942975133995\n",
            " epoch: 124  auc: 0.7632896933485633\n",
            " epoch: 125  auc: 0.7628503646428259\n",
            " epoch: 126  auc: 0.7625428345488093\n",
            " epoch: 127  auc: 0.762235304454793\n",
            " epoch: 128  auc: 0.7624110359370881\n",
            " epoch: 129  auc: 0.7626746331605307\n",
            " epoch: 130  auc: 0.7627185660311044\n",
            " epoch: 131  auc: 0.7630700289956946\n",
            " epoch: 132  auc: 0.7632457604779898\n",
            " epoch: 133  auc: 0.7630700289956945\n",
            " epoch: 134  auc: 0.7630260961251207\n",
            " epoch: 135  auc: 0.7634214919602847\n",
            " epoch: 136  auc: 0.7634214919602846\n",
            " epoch: 137  auc: 0.7632896933485633\n",
            " epoch: 138  auc: 0.7631578947368421\n",
            "fold: 1  epoch: 38\n",
            "fold: 1  accuracy: 0.9403453689167975\n",
            "fold: 1  f1_score: 0.0\n",
            "fold: 1  auc: 0.7948993937263861\n",
            " epoch: 0  auc: 0.427810791537913\n",
            " epoch: 1  auc: 0.4227240313762776\n",
            " epoch: 2  auc: 0.4204421202757309\n",
            " epoch: 3  auc: 0.4165913952935584\n",
            " epoch: 4  auc: 0.4157832184454481\n",
            " epoch: 5  auc: 0.4138340860470644\n",
            " epoch: 6  auc: 0.4112193962443546\n",
            " epoch: 7  auc: 0.4128832897551699\n",
            " epoch: 8  auc: 0.4135488471594961\n",
            " epoch: 9  auc: 0.4149750415973378\n",
            " epoch: 10  auc: 0.4154979795578797\n",
            " epoch: 11  auc: 0.41892084620869985\n",
            " epoch: 12  auc: 0.4226764915616829\n",
            " epoch: 13  auc: 0.42747801283574993\n",
            " epoch: 14  auc: 0.4330877109579273\n",
            " epoch: 15  auc: 0.4396006655574043\n",
            " epoch: 16  auc: 0.4473971951509389\n",
            " epoch: 17  auc: 0.4549084858569052\n",
            " epoch: 18  auc: 0.4650344663655811\n",
            " epoch: 19  auc: 0.4749702876158783\n",
            " epoch: 20  auc: 0.4862847634894224\n",
            " epoch: 21  auc: 0.49835987639648205\n",
            " epoch: 22  auc: 0.5107677680057048\n",
            " epoch: 23  auc: 0.5234846684097932\n",
            " epoch: 24  auc: 0.5357737104825291\n",
            " epoch: 25  auc: 0.5479439030187783\n",
            " epoch: 26  auc: 0.5613501307344901\n",
            " epoch: 27  auc: 0.5723318279058711\n",
            " epoch: 28  auc: 0.5833135250772522\n",
            " epoch: 29  auc: 0.5951984787259329\n",
            " epoch: 30  auc: 0.607891609222724\n",
            " epoch: 31  auc: 0.6176372712146422\n",
            " epoch: 32  auc: 0.6267649156168291\n",
            " epoch: 33  auc: 0.6339909674352271\n",
            " epoch: 34  auc: 0.6421202757309246\n",
            " epoch: 35  auc: 0.6487758497741859\n",
            " epoch: 36  auc: 0.6542191585452816\n",
            " epoch: 37  auc: 0.66009032564773\n",
            " epoch: 38  auc: 0.6643689089612551\n",
            " epoch: 39  auc: 0.6692655098645115\n",
            " epoch: 40  auc: 0.6739719515093892\n",
            " epoch: 41  auc: 0.6791062514856192\n",
            " epoch: 42  auc: 0.6832422153553601\n",
            " epoch: 43  auc: 0.6865700023769907\n",
            " epoch: 44  auc: 0.6900404088424055\n",
            " epoch: 45  auc: 0.6937485143807939\n",
            " epoch: 46  auc: 0.6966009032564773\n",
            " epoch: 47  auc: 0.6995483717613501\n",
            " epoch: 48  auc: 0.701117185642976\n",
            " epoch: 49  auc: 0.7036843356310911\n",
            " epoch: 50  auc: 0.7059187069170432\n",
            " epoch: 51  auc: 0.7079153791300213\n",
            " epoch: 52  auc: 0.7088186356073212\n",
            " epoch: 53  auc: 0.7113382457808414\n",
            " epoch: 54  auc: 0.7128119800332778\n",
            " epoch: 55  auc: 0.7140480152127406\n",
            " epoch: 56  auc: 0.7157594485381508\n",
            " epoch: 57  auc: 0.7166151652008558\n",
            " epoch: 58  auc: 0.7186593772284288\n",
            " epoch: 59  auc: 0.7188970763014025\n",
            " epoch: 60  auc: 0.7200380318516758\n",
            " epoch: 61  auc: 0.7206560494414073\n",
            " epoch: 62  auc: 0.7213691466603281\n",
            " epoch: 63  auc: 0.7219396244354647\n",
            " epoch: 64  auc: 0.7229855003565485\n",
            " epoch: 65  auc: 0.7238887568338482\n",
            " epoch: 66  auc: 0.7244116947943903\n",
            " epoch: 67  auc: 0.7248395531257428\n",
            " epoch: 68  auc: 0.7252674114570953\n",
            " epoch: 69  auc: 0.7256477299738531\n",
            " epoch: 70  auc: 0.726028048490611\n",
            " epoch: 71  auc: 0.7264083670073687\n",
            " epoch: 72  auc: 0.7265985262657475\n",
            " epoch: 73  auc: 0.726693605894937\n",
            " epoch: 74  auc: 0.7267411457095317\n",
            " epoch: 75  auc: 0.7271214642262895\n",
            " epoch: 76  auc: 0.7279771808889945\n",
            " epoch: 77  auc: 0.7283099595911575\n",
            " epoch: 78  auc: 0.728405039220347\n",
            " epoch: 79  auc: 0.728927977180889\n",
            " epoch: 80  auc: 0.7293082956976468\n",
            " epoch: 81  auc: 0.7294984549560256\n",
            " epoch: 82  auc: 0.7300213929165675\n",
            " epoch: 83  auc: 0.7308771095792727\n",
            " epoch: 84  auc: 0.731304967910625\n",
            " epoch: 85  auc: 0.7318279058711671\n",
            " epoch: 86  auc: 0.7318754456857618\n",
            " epoch: 87  auc: 0.7320656049441407\n",
            " epoch: 88  auc: 0.7328262419776562\n",
            " epoch: 89  auc: 0.73306394105063\n",
            " epoch: 90  auc: 0.7331114808652247\n",
            " epoch: 91  auc: 0.733396719752793\n",
            " epoch: 92  auc: 0.7335393391965771\n",
            " epoch: 93  auc: 0.7338245780841455\n",
            " epoch: 94  auc: 0.7343950558592821\n",
            " epoch: 95  auc: 0.7347753743760399\n",
            " epoch: 96  auc: 0.7347278345614452\n",
            " epoch: 97  auc: 0.7345376753030664\n",
            " epoch: 98  auc: 0.7347278345614452\n",
            " epoch: 99  auc: 0.7348229141906346\n",
            " epoch: 100  auc: 0.7346327549322558\n",
            " epoch: 101  auc: 0.7346327549322558\n",
            " epoch: 102  auc: 0.7347278345614452\n",
            " epoch: 103  auc: 0.7348704540052294\n",
            " epoch: 104  auc: 0.7350130734490136\n",
            " epoch: 105  auc: 0.7350130734490136\n",
            " epoch: 106  auc: 0.7354409317803661\n",
            " epoch: 107  auc: 0.7356310910387449\n",
            " epoch: 108  auc: 0.7359163299263133\n",
            " epoch: 109  auc: 0.7357261706679344\n",
            " epoch: 110  auc: 0.7354409317803662\n",
            " epoch: 111  auc: 0.7353933919657714\n",
            " epoch: 112  auc: 0.7357261706679343\n",
            " epoch: 113  auc: 0.7357261706679344\n",
            " epoch: 114  auc: 0.7353933919657714\n",
            " epoch: 115  auc: 0.7356786308533397\n",
            " epoch: 116  auc: 0.735963869740908\n",
            " epoch: 117  auc: 0.7360114095555027\n",
            " epoch: 118  auc: 0.7360589493700974\n",
            " epoch: 119  auc: 0.7360114095555027\n",
            " epoch: 120  auc: 0.735963869740908\n",
            " epoch: 121  auc: 0.7362015688138817\n",
            " epoch: 122  auc: 0.7363917280722605\n",
            " epoch: 123  auc: 0.7363441882576658\n",
            " epoch: 124  auc: 0.7364868077014499\n",
            " epoch: 125  auc: 0.7363441882576658\n",
            " epoch: 126  auc: 0.7361064891846921\n",
            " epoch: 127  auc: 0.7364392678868553\n",
            " epoch: 128  auc: 0.7363917280722605\n",
            " epoch: 129  auc: 0.736296648443071\n",
            " epoch: 130  auc: 0.7363441882576657\n",
            " epoch: 131  auc: 0.7362491086284764\n",
            " epoch: 132  auc: 0.7364392678868553\n",
            " epoch: 133  auc: 0.736629427145234\n",
            " epoch: 134  auc: 0.7366769669598289\n",
            " epoch: 135  auc: 0.7369146660328025\n",
            " epoch: 136  auc: 0.7369622058473971\n",
            " epoch: 137  auc: 0.7371523651057761\n",
            " epoch: 138  auc: 0.7366769669598289\n",
            " epoch: 139  auc: 0.7366769669598289\n",
            " epoch: 140  auc: 0.736819586403613\n",
            " epoch: 141  auc: 0.7367245067744236\n",
            " epoch: 142  auc: 0.7366294271452342\n",
            " epoch: 143  auc: 0.7368671262182077\n",
            " epoch: 144  auc: 0.7367720465890183\n",
            " epoch: 145  auc: 0.7365818873306395\n",
            " epoch: 146  auc: 0.7363917280722605\n",
            " epoch: 147  auc: 0.7363441882576658\n",
            " epoch: 148  auc: 0.7360589493700974\n",
            " epoch: 149  auc: 0.7362966484430711\n",
            " epoch: 150  auc: 0.7361540289992868\n",
            " epoch: 151  auc: 0.7362966484430711\n",
            " epoch: 152  auc: 0.7363917280722605\n",
            " epoch: 153  auc: 0.7363441882576658\n",
            " epoch: 154  auc: 0.7361064891846921\n",
            " epoch: 155  auc: 0.7362015688138817\n",
            " epoch: 156  auc: 0.7360589493700974\n",
            " epoch: 157  auc: 0.7361064891846921\n",
            " epoch: 158  auc: 0.7360114095555027\n",
            " epoch: 159  auc: 0.7359163299263133\n",
            " epoch: 160  auc: 0.7360114095555027\n",
            " epoch: 161  auc: 0.7358687901117186\n",
            " epoch: 162  auc: 0.7357261706679343\n",
            " epoch: 163  auc: 0.7354884715949608\n",
            " epoch: 164  auc: 0.7356310910387449\n",
            " epoch: 165  auc: 0.7357261706679344\n",
            " epoch: 166  auc: 0.7358687901117186\n",
            " epoch: 167  auc: 0.7360589493700975\n",
            " epoch: 168  auc: 0.7361064891846922\n",
            " epoch: 169  auc: 0.7361064891846922\n",
            " epoch: 170  auc: 0.7362491086284765\n",
            " epoch: 171  auc: 0.7363441882576659\n",
            " epoch: 172  auc: 0.7363441882576658\n",
            " epoch: 173  auc: 0.7362966484430711\n",
            " epoch: 174  auc: 0.7362491086284764\n",
            " epoch: 175  auc: 0.7363917280722606\n",
            " epoch: 176  auc: 0.7361064891846922\n",
            " epoch: 177  auc: 0.7363441882576658\n",
            " epoch: 178  auc: 0.7361540289992868\n",
            " epoch: 179  auc: 0.7358212502971239\n",
            " epoch: 180  auc: 0.7355360114095554\n",
            " epoch: 181  auc: 0.7354884715949608\n",
            " epoch: 182  auc: 0.7354884715949608\n",
            " epoch: 183  auc: 0.7356310910387449\n",
            " epoch: 184  auc: 0.7355835512241502\n",
            " epoch: 185  auc: 0.7354884715949607\n",
            " epoch: 186  auc: 0.7353933919657714\n",
            " epoch: 187  auc: 0.7353933919657712\n",
            " epoch: 188  auc: 0.735440931780366\n",
            " epoch: 189  auc: 0.7352507725219871\n",
            " epoch: 190  auc: 0.735108153078203\n",
            " epoch: 191  auc: 0.7352032327073924\n",
            " epoch: 192  auc: 0.7352983123365818\n",
            " epoch: 193  auc: 0.7352507725219871\n",
            " epoch: 194  auc: 0.7351556928927977\n",
            " epoch: 195  auc: 0.735108153078203\n",
            " epoch: 196  auc: 0.734917993819824\n",
            " epoch: 197  auc: 0.7349179938198241\n",
            " epoch: 198  auc: 0.7348229141906346\n",
            " epoch: 199  auc: 0.7343950558592821\n",
            " epoch: 200  auc: 0.7347278345614453\n",
            " epoch: 201  auc: 0.7344425956738768\n",
            " epoch: 202  auc: 0.734252436415498\n",
            " epoch: 203  auc: 0.7341573567863086\n",
            " epoch: 204  auc: 0.7338245780841455\n",
            " epoch: 205  auc: 0.7337770382695508\n",
            " epoch: 206  auc: 0.7334917993819824\n",
            " epoch: 207  auc: 0.7333016401236034\n",
            " epoch: 208  auc: 0.7331590206798193\n",
            " epoch: 209  auc: 0.7327787021630615\n",
            " epoch: 210  auc: 0.7326836225338721\n",
            " epoch: 211  auc: 0.7324934632754931\n",
            " epoch: 212  auc: 0.7320656049441406\n",
            " epoch: 213  auc: 0.7321131447587353\n",
            " epoch: 214  auc: 0.7317328262419777\n",
            " epoch: 215  auc: 0.7316377466127882\n",
            " epoch: 216  auc: 0.731685286427383\n",
            " epoch: 217  auc: 0.7314475873544093\n",
            " epoch: 218  auc: 0.7315902067981935\n",
            " epoch: 219  auc: 0.7315426669835987\n",
            " epoch: 220  auc: 0.7314000475398146\n",
            " epoch: 221  auc: 0.7314475873544093\n",
            " epoch: 222  auc: 0.7313525077252199\n",
            " epoch: 223  auc: 0.7310672688376516\n",
            " epoch: 224  auc: 0.7312098882814357\n",
            " epoch: 225  auc: 0.7312098882814356\n",
            " epoch: 226  auc: 0.7313525077252199\n",
            " epoch: 227  auc: 0.7311148086522463\n",
            " epoch: 228  auc: 0.7307344901354885\n",
            " epoch: 229  auc: 0.7308295697646779\n",
            " epoch: 230  auc: 0.7308771095792728\n",
            " epoch: 231  auc: 0.7304492512479202\n",
            " epoch: 232  auc: 0.7304017114333254\n",
            " epoch: 233  auc: 0.7302590919895413\n",
            " epoch: 234  auc: 0.7298312336581887\n",
            " epoch: 235  auc: 0.7297361540289993\n",
            " epoch: 236  auc: 0.7295459947706203\n",
            " epoch: 237  auc: 0.7295935345852151\n",
            "fold: 2  epoch: 137\n",
            "fold: 2  accuracy: 0.9449685534591195\n",
            "fold: 2  f1_score: 0.05405405405405405\n",
            "fold: 2  auc: 0.7371523651057761\n",
            " epoch: 0  auc: 0.36635563849640096\n",
            " epoch: 1  auc: 0.37984537456678225\n",
            " epoch: 2  auc: 0.39408157824580115\n",
            " epoch: 3  auc: 0.40261263663023195\n",
            " epoch: 4  auc: 0.3981338309784057\n",
            " epoch: 5  auc: 0.4101839509464143\n",
            " epoch: 6  auc: 0.4261263663023194\n",
            " epoch: 7  auc: 0.4367368701679552\n",
            " epoch: 8  auc: 0.43966942148760335\n",
            " epoch: 9  auc: 0.4464942681951479\n",
            " epoch: 10  auc: 0.4508664356171687\n",
            " epoch: 11  auc: 0.4535857104772061\n",
            " epoch: 12  auc: 0.45785123966942154\n",
            " epoch: 13  auc: 0.46632897893894965\n",
            " epoch: 14  auc: 0.473900293255132\n",
            " epoch: 15  auc: 0.4844041588909624\n",
            " epoch: 16  auc: 0.49722740602505994\n",
            " epoch: 17  auc: 0.5083444414822714\n",
            " epoch: 18  auc: 0.5196480938416422\n",
            " epoch: 19  auc: 0.5308451079712077\n",
            " epoch: 20  auc: 0.5450279925353239\n",
            " epoch: 21  auc: 0.5556918155158624\n",
            " epoch: 22  auc: 0.5673153825646494\n",
            " epoch: 23  auc: 0.578832311383631\n",
            " epoch: 24  auc: 0.5881098373766995\n",
            " epoch: 25  auc: 0.5998400426552919\n",
            " epoch: 26  auc: 0.6086376966142362\n",
            " epoch: 27  auc: 0.6189816049053586\n",
            " epoch: 28  auc: 0.6290589176219674\n",
            " epoch: 29  auc: 0.6397227406025061\n",
            " epoch: 30  auc: 0.6463876299653426\n",
            " epoch: 31  auc: 0.6563049853372434\n",
            " epoch: 32  auc: 0.6644094908024527\n",
            " epoch: 33  auc: 0.6721407624633431\n",
            " epoch: 34  auc: 0.6794988003199146\n",
            " epoch: 35  auc: 0.6870167955211943\n",
            " epoch: 36  auc: 0.6928285790455877\n",
            " epoch: 37  auc: 0.6974140229272193\n",
            " epoch: 38  auc: 0.701146360970408\n",
            " epoch: 39  auc: 0.7043455078645695\n",
            " epoch: 40  auc: 0.7086643561716875\n",
            " epoch: 41  auc: 0.7125033324446814\n",
            " epoch: 42  auc: 0.7165555851772861\n",
            " epoch: 43  auc: 0.7190615835777127\n",
            " epoch: 44  auc: 0.7206078379098907\n",
            " epoch: 45  auc: 0.7223140495867768\n",
            " epoch: 46  auc: 0.7241268994934684\n",
            " epoch: 47  auc: 0.7257264729405491\n",
            " epoch: 48  auc: 0.727645961077046\n",
            " epoch: 49  auc: 0.7286057051452945\n",
            " epoch: 50  auc: 0.7291388962943215\n",
            " epoch: 51  auc: 0.7298320447880564\n",
            " epoch: 52  auc: 0.7306318315115968\n",
            " epoch: 53  auc: 0.7317515329245534\n",
            " epoch: 54  auc: 0.7316982138096507\n",
            " epoch: 55  auc: 0.7324446814182884\n",
            " epoch: 56  auc: 0.7330311916822181\n",
            " epoch: 57  auc: 0.7334577446014396\n",
            " epoch: 58  auc: 0.733564382831245\n",
            " epoch: 59  auc: 0.7339909357504666\n",
            " epoch: 60  auc: 0.7341508930951746\n",
            " epoch: 61  auc: 0.7347374033591043\n",
            " epoch: 62  auc: 0.7344708077845907\n",
            " epoch: 63  auc: 0.7342575313249801\n",
            " epoch: 64  auc: 0.7344708077845907\n",
            " epoch: 65  auc: 0.7345774460143962\n",
            " epoch: 66  auc: 0.7344174886696881\n",
            " epoch: 67  auc: 0.7345774460143962\n",
            " epoch: 68  auc: 0.7344174886696879\n",
            " epoch: 69  auc: 0.734417488669688\n",
            " epoch: 70  auc: 0.73425753132498\n",
            " epoch: 71  auc: 0.7346307651292989\n",
            " epoch: 72  auc: 0.7347374033591041\n",
            " epoch: 73  auc: 0.734950679818715\n",
            " epoch: 74  auc: 0.7356971474273527\n",
            " epoch: 75  auc: 0.7352705945081311\n",
            " epoch: 76  auc: 0.7347907224740069\n",
            " epoch: 77  auc: 0.7348440415889097\n",
            " epoch: 78  auc: 0.735270594508131\n",
            " epoch: 79  auc: 0.7352172753932285\n",
            " epoch: 80  auc: 0.7355905091975472\n",
            " epoch: 81  auc: 0.7352172753932285\n",
            " epoch: 82  auc: 0.7351106371634231\n",
            " epoch: 83  auc: 0.7350573180485204\n",
            " epoch: 84  auc: 0.7348973607038123\n",
            " epoch: 85  auc: 0.7345774460143961\n",
            " epoch: 86  auc: 0.7348973607038123\n",
            " epoch: 87  auc: 0.7352172753932285\n",
            " epoch: 88  auc: 0.735270594508131\n",
            " epoch: 89  auc: 0.735270594508131\n",
            " epoch: 90  auc: 0.7355905091975472\n",
            " epoch: 91  auc: 0.735803785657158\n",
            " epoch: 92  auc: 0.7360703812316715\n",
            " epoch: 93  auc: 0.7363369768061849\n",
            " epoch: 94  auc: 0.7367102106105039\n",
            " epoch: 95  auc: 0.7369234870701145\n",
            " epoch: 96  auc: 0.7366568914956011\n",
            " epoch: 97  auc: 0.7371367635297253\n",
            " epoch: 98  auc: 0.7370834444148227\n",
            " epoch: 99  auc: 0.7372967208744334\n",
            " epoch: 100  auc: 0.7371900826446282\n",
            " epoch: 101  auc: 0.7369234870701146\n",
            " epoch: 102  auc: 0.7367635297254066\n",
            " epoch: 103  auc: 0.7367102106105038\n",
            " epoch: 104  auc: 0.7374566782191415\n",
            " epoch: 105  auc: 0.7379365502532658\n",
            " epoch: 106  auc: 0.7386296987470008\n",
            " epoch: 107  auc: 0.738949613436417\n",
            " epoch: 108  auc: 0.7387896560917088\n",
            " epoch: 109  auc: 0.7388429752066116\n",
            " epoch: 110  auc: 0.7390562516662224\n",
            " epoch: 111  auc: 0.738949613436417\n",
            " epoch: 112  auc: 0.739269528125833\n",
            " epoch: 113  auc: 0.7392695281258331\n",
            " epoch: 114  auc: 0.7394828045854438\n",
            " epoch: 115  auc: 0.7392162090109303\n",
            " epoch: 116  auc: 0.739109570781125\n",
            " epoch: 117  auc: 0.7385763796320981\n",
            " epoch: 118  auc: 0.7388962943215142\n",
            " epoch: 119  auc: 0.7387363369768062\n",
            " epoch: 120  auc: 0.7385230605171954\n",
            " epoch: 121  auc: 0.7383631031724873\n",
            " epoch: 122  auc: 0.7385230605171955\n",
            " epoch: 123  auc: 0.7387363369768062\n",
            " epoch: 124  auc: 0.7386296987470008\n",
            " epoch: 125  auc: 0.7387096774193548\n",
            " epoch: 126  auc: 0.7386296987470008\n",
            " epoch: 127  auc: 0.73841642228739\n",
            " epoch: 128  auc: 0.737723273793655\n",
            " epoch: 129  auc: 0.7374566782191416\n",
            " epoch: 130  auc: 0.7369768061850173\n",
            " epoch: 131  auc: 0.7366568914956012\n",
            " epoch: 132  auc: 0.7364969341508931\n",
            " epoch: 133  auc: 0.7359104238869635\n",
            " epoch: 134  auc: 0.7357504665422554\n",
            " epoch: 135  auc: 0.7354305518528392\n",
            " epoch: 136  auc: 0.7350039989336177\n",
            " epoch: 137  auc: 0.734950679818715\n",
            " epoch: 138  auc: 0.7351106371634231\n",
            " epoch: 139  auc: 0.7349506798187149\n",
            " epoch: 140  auc: 0.7345241268994934\n",
            " epoch: 141  auc: 0.7347374033591042\n",
            " epoch: 142  auc: 0.7347907224740069\n",
            " epoch: 143  auc: 0.7346307651292988\n",
            " epoch: 144  auc: 0.7344174886696881\n",
            " epoch: 145  auc: 0.7344708077845907\n",
            " epoch: 146  auc: 0.734417488669688\n",
            " epoch: 147  auc: 0.7341508930951746\n",
            " epoch: 148  auc: 0.7339909357504665\n",
            " epoch: 149  auc: 0.7337776592908557\n",
            " epoch: 150  auc: 0.7336710210610503\n",
            " epoch: 151  auc: 0.7333511063716341\n",
            " epoch: 152  auc: 0.7331911490269262\n",
            " epoch: 153  auc: 0.7331911490269261\n",
            " epoch: 154  auc: 0.7328712343375099\n",
            " epoch: 155  auc: 0.73287123433751\n",
            " epoch: 156  auc: 0.7329778725673153\n",
            " epoch: 157  auc: 0.7327645961077046\n",
            " epoch: 158  auc: 0.7328712343375099\n",
            " epoch: 159  auc: 0.7324980005331911\n",
            " epoch: 160  auc: 0.7323913623033858\n",
            " epoch: 161  auc: 0.7319648093841642\n",
            " epoch: 162  auc: 0.7318581711543589\n",
            " epoch: 163  auc: 0.7314316182351374\n",
            " epoch: 164  auc: 0.7312183417755265\n",
            " epoch: 165  auc: 0.7312183417755265\n",
            " epoch: 166  auc: 0.7312183417755265\n",
            " epoch: 167  auc: 0.7306318315115968\n",
            " epoch: 168  auc: 0.7303652359370834\n",
            " epoch: 169  auc: 0.7305251932817916\n",
            " epoch: 170  auc: 0.7303119168221807\n",
            " epoch: 171  auc: 0.7302052785923753\n",
            " epoch: 172  auc: 0.7302052785923755\n",
            " epoch: 173  auc: 0.7300453212476673\n",
            " epoch: 174  auc: 0.7300453212476672\n",
            " epoch: 175  auc: 0.7298853639029592\n",
            " epoch: 176  auc: 0.7298587043455079\n",
            " epoch: 177  auc: 0.7295121300986402\n",
            " epoch: 178  auc: 0.7290322580645161\n",
            " epoch: 179  auc: 0.7289789389496134\n",
            " epoch: 180  auc: 0.7286590242601972\n",
            " epoch: 181  auc: 0.7280191948813649\n",
            " epoch: 182  auc: 0.7273793655025327\n",
            " epoch: 183  auc: 0.7272194081578246\n",
            " epoch: 184  auc: 0.7270061316982138\n",
            " epoch: 185  auc: 0.7268461743535057\n",
            " epoch: 186  auc: 0.7265795787789923\n",
            " epoch: 187  auc: 0.7264196214342842\n",
            " epoch: 188  auc: 0.7262063449746735\n",
            " epoch: 189  auc: 0.7259930685150626\n",
            " epoch: 190  auc: 0.7260463876299654\n",
            " epoch: 191  auc: 0.7258864302852572\n",
            " epoch: 192  auc: 0.7255665155958411\n",
            " epoch: 193  auc: 0.7256198347107438\n",
            " epoch: 194  auc: 0.7256198347107438\n",
            " epoch: 195  auc: 0.7254598773660357\n",
            " epoch: 196  auc: 0.7252466009064249\n",
            " epoch: 197  auc: 0.7250866435617168\n",
            " epoch: 198  auc: 0.7247667288723006\n",
            " epoch: 199  auc: 0.7249266862170087\n",
            " epoch: 200  auc: 0.7248733671021063\n",
            " epoch: 201  auc: 0.72455345241269\n",
            " epoch: 202  auc: 0.7244468141828846\n",
            " epoch: 203  auc: 0.7240202612636629\n",
            " epoch: 204  auc: 0.7239136230338576\n",
            " epoch: 205  auc: 0.7236470274593442\n",
            " epoch: 206  auc: 0.7235937083444415\n",
            " epoch: 207  auc: 0.7236470274593442\n",
            " epoch: 208  auc: 0.7233804318848307\n",
            " epoch: 209  auc: 0.723327112769928\n",
            " epoch: 210  auc: 0.7233804318848307\n",
            " epoch: 211  auc: 0.7232204745401227\n",
            " epoch: 212  auc: 0.7232204745401226\n",
            " epoch: 213  auc: 0.7231671554252199\n",
            " epoch: 214  auc: 0.7230071980805117\n",
            "fold: 3  epoch: 114\n",
            "fold: 3  accuracy: 0.9512578616352201\n",
            "fold: 3  f1_score: 0.0\n",
            "fold: 3  auc: 0.7394828045854438\n",
            " epoch: 0  auc: 0.4455536912751678\n",
            " epoch: 1  auc: 0.4336828859060402\n",
            " epoch: 2  auc: 0.43162751677852346\n",
            " epoch: 3  auc: 0.4356963087248322\n",
            " epoch: 4  auc: 0.43640939597315437\n",
            " epoch: 5  auc: 0.44211409395973156\n",
            " epoch: 6  auc: 0.44421140939597314\n",
            " epoch: 7  auc: 0.44702181208053693\n",
            " epoch: 8  auc: 0.451258389261745\n",
            " epoch: 9  auc: 0.4523070469798658\n",
            " epoch: 10  auc: 0.45281040268456374\n",
            " epoch: 11  auc: 0.4518875838926175\n",
            " epoch: 12  auc: 0.45385906040268453\n",
            " epoch: 13  auc: 0.46153523489932885\n",
            " epoch: 14  auc: 0.46535234899328853\n",
            " epoch: 15  auc: 0.46719798657718126\n",
            " epoch: 16  auc: 0.47093120805369126\n",
            " epoch: 17  auc: 0.4745385906040269\n",
            " epoch: 18  auc: 0.4802013422818792\n",
            " epoch: 19  auc: 0.48640939597315436\n",
            " epoch: 20  auc: 0.49223993288590606\n",
            " epoch: 21  auc: 0.49899328859060393\n",
            " epoch: 22  auc: 0.5105914429530202\n",
            " epoch: 23  auc: 0.520008389261745\n",
            " epoch: 24  auc: 0.5298657718120805\n",
            " epoch: 25  auc: 0.5411073825503356\n",
            " epoch: 26  auc: 0.5552013422818792\n",
            " epoch: 27  auc: 0.5677432885906041\n",
            " epoch: 28  auc: 0.5815855704697988\n",
            " epoch: 29  auc: 0.5933305369127517\n",
            " epoch: 30  auc: 0.6046979865771812\n",
            " epoch: 31  auc: 0.6156879194630873\n",
            " epoch: 32  auc: 0.6264261744966443\n",
            " epoch: 33  auc: 0.6371644295302014\n",
            " epoch: 34  auc: 0.6472315436241611\n",
            " epoch: 35  auc: 0.6545302013422819\n",
            " epoch: 36  auc: 0.6614093959731544\n",
            " epoch: 37  auc: 0.6702600671140939\n",
            " epoch: 38  auc: 0.675755033557047\n",
            " epoch: 39  auc: 0.6805788590604027\n",
            " epoch: 40  auc: 0.685989932885906\n",
            " epoch: 41  auc: 0.6901426174496644\n",
            " epoch: 42  auc: 0.6933724832214765\n",
            " epoch: 43  auc: 0.6972315436241611\n",
            " epoch: 44  auc: 0.7001258389261744\n",
            " epoch: 45  auc: 0.7023070469798657\n",
            " epoch: 46  auc: 0.703481543624161\n",
            " epoch: 47  auc: 0.7044043624161074\n",
            " epoch: 48  auc: 0.7056627516778524\n",
            " epoch: 49  auc: 0.7067114093959732\n",
            " epoch: 50  auc: 0.7071728187919464\n",
            " epoch: 51  auc: 0.7071308724832215\n",
            " epoch: 52  auc: 0.708263422818792\n",
            " epoch: 53  auc: 0.7093540268456376\n",
            " epoch: 54  auc: 0.7097315436241611\n",
            " epoch: 55  auc: 0.7102348993288591\n",
            " epoch: 56  auc: 0.7103187919463088\n",
            " epoch: 57  auc: 0.7105285234899329\n",
            " epoch: 58  auc: 0.7105704697986577\n",
            " epoch: 59  auc: 0.7109060402684564\n",
            " epoch: 60  auc: 0.7111157718120805\n",
            " epoch: 61  auc: 0.7110318791946308\n",
            " epoch: 62  auc: 0.7111577181208054\n",
            " epoch: 63  auc: 0.7113255033557048\n",
            " epoch: 64  auc: 0.7111577181208053\n",
            " epoch: 65  auc: 0.7116191275167785\n",
            " epoch: 66  auc: 0.7119127516778523\n",
            " epoch: 67  auc: 0.7124580536912752\n",
            " epoch: 68  auc: 0.7129194630872483\n",
            " epoch: 69  auc: 0.7131711409395973\n",
            " epoch: 70  auc: 0.7132130872483222\n",
            " epoch: 71  auc: 0.7135486577181208\n",
            " epoch: 72  auc: 0.7139681208053692\n",
            " epoch: 73  auc: 0.7140520134228188\n",
            " epoch: 74  auc: 0.7145134228187919\n",
            " epoch: 75  auc: 0.7147231543624161\n",
            " epoch: 76  auc: 0.7150167785234899\n",
            " epoch: 77  auc: 0.7153104026845638\n",
            " epoch: 78  auc: 0.7156879194630873\n",
            " epoch: 79  auc: 0.7159815436241611\n",
            " epoch: 80  auc: 0.7164848993288591\n",
            " epoch: 81  auc: 0.7167365771812081\n",
            " epoch: 82  auc: 0.717239932885906\n",
            " epoch: 83  auc: 0.7178271812080537\n",
            " epoch: 84  auc: 0.718498322147651\n",
            " epoch: 85  auc: 0.7190436241610738\n",
            " epoch: 86  auc: 0.7195889261744967\n",
            " epoch: 87  auc: 0.7200503355704698\n",
            " epoch: 88  auc: 0.7206795302013422\n",
            " epoch: 89  auc: 0.7210570469798658\n",
            " epoch: 90  auc: 0.721728187919463\n",
            " epoch: 91  auc: 0.722273489932886\n",
            " epoch: 92  auc: 0.7227768456375839\n",
            " epoch: 93  auc: 0.7236996644295302\n",
            " epoch: 94  auc: 0.7242869127516778\n",
            " epoch: 95  auc: 0.7246644295302013\n",
            " epoch: 96  auc: 0.725251677852349\n",
            " epoch: 97  auc: 0.7255872483221477\n",
            " epoch: 98  auc: 0.7256291946308724\n",
            " epoch: 99  auc: 0.7261325503355704\n",
            " epoch: 100  auc: 0.7265939597315436\n",
            " epoch: 101  auc: 0.7269295302013423\n",
            " epoch: 102  auc: 0.7273489932885906\n",
            " epoch: 103  auc: 0.7274328859060404\n",
            " epoch: 104  auc: 0.72751677852349\n",
            " epoch: 105  auc: 0.7278104026845638\n",
            " epoch: 106  auc: 0.7277684563758389\n",
            " epoch: 107  auc: 0.7279781879194631\n",
            " epoch: 108  auc: 0.7283137583892617\n",
            " epoch: 109  auc: 0.7280201342281879\n",
            " epoch: 110  auc: 0.7278942953020133\n",
            " epoch: 111  auc: 0.7279362416107382\n",
            " epoch: 112  auc: 0.7283557046979866\n",
            " epoch: 113  auc: 0.7287751677852349\n",
            " epoch: 114  auc: 0.7288590604026846\n",
            " epoch: 115  auc: 0.7290268456375839\n",
            " epoch: 116  auc: 0.7289010067114094\n",
            " epoch: 117  auc: 0.7290687919463087\n",
            " epoch: 118  auc: 0.7295302013422819\n",
            " epoch: 119  auc: 0.7294882550335571\n",
            " epoch: 120  auc: 0.7299077181208053\n",
            " epoch: 121  auc: 0.7302852348993288\n",
            " epoch: 122  auc: 0.730746644295302\n",
            " epoch: 123  auc: 0.7310822147651007\n",
            " epoch: 124  auc: 0.730998322147651\n",
            " epoch: 125  auc: 0.7312080536912751\n",
            " epoch: 126  auc: 0.7314597315436242\n",
            " epoch: 127  auc: 0.731963087248322\n",
            " epoch: 128  auc: 0.7317953020134228\n",
            " epoch: 129  auc: 0.7314177852348993\n",
            " epoch: 130  auc: 0.731501677852349\n",
            " epoch: 131  auc: 0.7315016778523489\n",
            " epoch: 132  auc: 0.7315855704697987\n",
            " epoch: 133  auc: 0.7314177852348994\n",
            " epoch: 134  auc: 0.7313758389261744\n",
            " epoch: 135  auc: 0.7315855704697987\n",
            " epoch: 136  auc: 0.7315016778523489\n",
            " epoch: 137  auc: 0.7313338926174497\n",
            " epoch: 138  auc: 0.7311241610738255\n",
            " epoch: 139  auc: 0.7309144295302014\n",
            " epoch: 140  auc: 0.730746644295302\n",
            " epoch: 141  auc: 0.7307046979865772\n",
            " epoch: 142  auc: 0.730998322147651\n",
            " epoch: 143  auc: 0.7309563758389261\n",
            " epoch: 144  auc: 0.7308305369127517\n",
            " epoch: 145  auc: 0.7307885906040269\n",
            " epoch: 146  auc: 0.7302013422818792\n",
            " epoch: 147  auc: 0.7303271812080536\n",
            " epoch: 148  auc: 0.7301593959731544\n",
            " epoch: 149  auc: 0.7303271812080537\n",
            " epoch: 150  auc: 0.7302852348993288\n",
            " epoch: 151  auc: 0.7302013422818792\n",
            " epoch: 152  auc: 0.7301174496644295\n",
            " epoch: 153  auc: 0.729739932885906\n",
            " epoch: 154  auc: 0.7297818791946309\n",
            " epoch: 155  auc: 0.7298238255033557\n",
            " epoch: 156  auc: 0.7295302013422819\n",
            " epoch: 157  auc: 0.7295721476510066\n",
            " epoch: 158  auc: 0.7296140939597315\n",
            " epoch: 159  auc: 0.729739932885906\n",
            " epoch: 160  auc: 0.7294463087248322\n",
            " epoch: 161  auc: 0.7289429530201341\n",
            " epoch: 162  auc: 0.7290687919463087\n",
            " epoch: 163  auc: 0.7291107382550335\n",
            " epoch: 164  auc: 0.7289010067114094\n",
            " epoch: 165  auc: 0.7289429530201342\n",
            " epoch: 166  auc: 0.7283976510067114\n",
            " epoch: 167  auc: 0.728229865771812\n",
            " epoch: 168  auc: 0.7281040268456376\n",
            " epoch: 169  auc: 0.727768456375839\n",
            " epoch: 170  auc: 0.727474832214765\n",
            " epoch: 171  auc: 0.7273070469798658\n",
            " epoch: 172  auc: 0.726761744966443\n",
            " epoch: 173  auc: 0.7263003355704698\n",
            " epoch: 174  auc: 0.7264261744966443\n",
            " epoch: 175  auc: 0.7264261744966443\n",
            " epoch: 176  auc: 0.7260486577181208\n",
            " epoch: 177  auc: 0.7258389261744966\n",
            " epoch: 178  auc: 0.7254194630872484\n",
            " epoch: 179  auc: 0.7251677852348993\n",
            " epoch: 180  auc: 0.724748322147651\n",
            " epoch: 181  auc: 0.7246224832214765\n",
            " epoch: 182  auc: 0.724496644295302\n",
            " epoch: 183  auc: 0.7242869127516778\n",
            " epoch: 184  auc: 0.7239093959731544\n",
            " epoch: 185  auc: 0.7236577181208054\n",
            " epoch: 186  auc: 0.7233640939597316\n",
            " epoch: 187  auc: 0.723238255033557\n",
            " epoch: 188  auc: 0.7228187919463087\n",
            " epoch: 189  auc: 0.7226090604026846\n",
            " epoch: 190  auc: 0.7225251677852349\n",
            " epoch: 191  auc: 0.7223573825503355\n",
            " epoch: 192  auc: 0.7219798657718121\n",
            " epoch: 193  auc: 0.7218120805369127\n",
            " epoch: 194  auc: 0.7218120805369127\n",
            " epoch: 195  auc: 0.7213506711409396\n",
            " epoch: 196  auc: 0.7209731543624162\n",
            " epoch: 197  auc: 0.7207214765100671\n",
            " epoch: 198  auc: 0.720260067114094\n",
            " epoch: 199  auc: 0.7200922818791946\n",
            " epoch: 200  auc: 0.7200083892617449\n",
            " epoch: 201  auc: 0.7197986577181207\n",
            " epoch: 202  auc: 0.719756711409396\n",
            " epoch: 203  auc: 0.7193372483221476\n",
            " epoch: 204  auc: 0.7190855704697987\n",
            " epoch: 205  auc: 0.7186661073825503\n",
            " epoch: 206  auc: 0.718246644295302\n",
            " epoch: 207  auc: 0.7181208053691275\n",
            " epoch: 208  auc: 0.717994966442953\n",
            " epoch: 209  auc: 0.7178271812080537\n",
            " epoch: 210  auc: 0.7176174496644294\n",
            " epoch: 211  auc: 0.7173657718120805\n",
            " epoch: 212  auc: 0.7168624161073825\n",
            " epoch: 213  auc: 0.7164848993288591\n",
            " epoch: 214  auc: 0.7160234899328859\n",
            " epoch: 215  auc: 0.7155201342281879\n",
            " epoch: 216  auc: 0.7151635906040268\n",
            " epoch: 217  auc: 0.7145973154362415\n",
            " epoch: 218  auc: 0.7142197986577181\n",
            " epoch: 219  auc: 0.7139681208053692\n",
            " epoch: 220  auc: 0.7134228187919464\n",
            " epoch: 221  auc: 0.7133389261744967\n",
            " epoch: 222  auc: 0.7128355704697986\n",
            " epoch: 223  auc: 0.7125419463087248\n",
            " epoch: 224  auc: 0.7123322147651007\n",
            " epoch: 225  auc: 0.7122063758389262\n",
            " epoch: 226  auc: 0.7121224832214764\n",
            " epoch: 227  auc: 0.7117869127516779\n",
            "fold: 4  epoch: 127\n",
            "fold: 4  accuracy: 0.9355345911949685\n",
            "fold: 4  f1_score: 0.0\n",
            "fold: 4  auc: 0.731963087248322\n",
            " epoch: 0  auc: 0.3784306146115191\n",
            " epoch: 1  auc: 0.3717733968990251\n",
            " epoch: 2  auc: 0.3807928531546622\n",
            " epoch: 3  auc: 0.38607567753296396\n",
            " epoch: 4  auc: 0.3904136064940085\n",
            " epoch: 5  auc: 0.38921101232659017\n",
            " epoch: 6  auc: 0.3929046944122321\n",
            " epoch: 7  auc: 0.39702787441480913\n",
            " epoch: 8  auc: 0.40308379504359404\n",
            " epoch: 9  auc: 0.40694927629600997\n",
            " epoch: 10  auc: 0.4109006571318129\n",
            " epoch: 11  auc: 0.4167418288021304\n",
            " epoch: 12  auc: 0.4242580423484946\n",
            " epoch: 13  auc: 0.4345230425632436\n",
            " epoch: 14  auc: 0.442812352360091\n",
            " epoch: 15  auc: 0.45277670403298553\n",
            " epoch: 16  auc: 0.46420134862345913\n",
            " epoch: 17  auc: 0.4753682944637718\n",
            " epoch: 18  auc: 0.48636344113731045\n",
            " epoch: 19  auc: 0.4986470815616544\n",
            " epoch: 20  auc: 0.511403169694627\n",
            " epoch: 21  auc: 0.5239015590774385\n",
            " epoch: 22  auc: 0.5373018940858136\n",
            " epoch: 23  auc: 0.5535798651376541\n",
            " epoch: 24  auc: 0.5665936520207877\n",
            " epoch: 25  auc: 0.5807456083837993\n",
            " epoch: 26  auc: 0.5933943220375381\n",
            " epoch: 27  auc: 0.6061933599622042\n",
            " epoch: 28  auc: 0.6186917493450157\n",
            " epoch: 29  auc: 0.6311042391444401\n",
            " epoch: 30  auc: 0.6429154318601554\n",
            " epoch: 31  auc: 0.654898423742645\n",
            " epoch: 32  auc: 0.6662371687497315\n",
            " epoch: 33  auc: 0.6753854743804493\n",
            " epoch: 34  auc: 0.6839324829274578\n",
            " epoch: 35  auc: 0.6921788429326117\n",
            " epoch: 36  auc: 0.7003393033543788\n",
            " epoch: 37  auc: 0.707769617317356\n",
            " epoch: 38  auc: 0.7129665421122707\n",
            " epoch: 39  auc: 0.7191513121161363\n",
            " epoch: 40  auc: 0.7249495339947601\n",
            " epoch: 41  auc: 0.7298887600395138\n",
            " epoch: 42  auc: 0.7345702873341065\n",
            " epoch: 43  auc: 0.7392088648370057\n",
            " epoch: 44  auc: 0.7428166473392604\n",
            " epoch: 45  auc: 0.7471116265086114\n",
            " epoch: 46  auc: 0.7504187604690118\n",
            " epoch: 47  auc: 0.7532534467207834\n",
            " epoch: 48  auc: 0.7559592835974746\n",
            " epoch: 49  auc: 0.7574195765150538\n",
            " epoch: 50  auc: 0.7590516685994073\n",
            " epoch: 51  auc: 0.7608126100588413\n",
            " epoch: 52  auc: 0.7617145556844049\n",
            " epoch: 53  auc: 0.7636043465189194\n",
            " epoch: 54  auc: 0.7648069406863377\n",
            " epoch: 55  auc: 0.7656659365202079\n",
            " epoch: 56  auc: 0.7663960829789975\n",
            " epoch: 57  auc: 0.7674268779796418\n",
            " epoch: 58  auc: 0.768028175063351\n",
            " epoch: 59  auc: 0.7692737190224627\n",
            " epoch: 60  auc: 0.7701756646480264\n",
            " epoch: 61  auc: 0.7706481123566551\n",
            " epoch: 62  auc: 0.7716789073572993\n",
            " epoch: 63  auc: 0.7718936563157669\n",
            " epoch: 64  auc: 0.7730962504831852\n",
            " epoch: 65  auc: 0.7730962504831852\n",
            " epoch: 66  auc: 0.7733539492333462\n",
            " epoch: 67  auc: 0.7736975475668944\n",
            " epoch: 68  auc: 0.7738693467336684\n",
            " epoch: 69  auc: 0.7739552463170554\n",
            " epoch: 70  auc: 0.774169995275523\n",
            " epoch: 71  auc: 0.7741270454838294\n",
            " epoch: 72  auc: 0.7745565434007645\n",
            " epoch: 73  auc: 0.7747283425675385\n",
            " epoch: 74  auc: 0.7751578404844737\n",
            " epoch: 75  auc: 0.7751148906927801\n",
            " epoch: 76  auc: 0.7756732379847958\n",
            " epoch: 77  auc: 0.7757591375681827\n",
            " epoch: 78  auc: 0.7759738865266504\n",
            " epoch: 79  auc: 0.7761886354851179\n",
            " epoch: 80  auc: 0.7764033844435854\n",
            " epoch: 81  auc: 0.77670403298544\n",
            " epoch: 82  auc: 0.7770905811106816\n",
            " epoch: 83  auc: 0.7773053300691491\n",
            " epoch: 84  auc: 0.7774341794442298\n",
            " epoch: 85  auc: 0.7774771292359233\n",
            " epoch: 86  auc: 0.7775630288193103\n",
            " epoch: 87  auc: 0.7776059786110038\n",
            " epoch: 88  auc: 0.7777777777777778\n",
            " epoch: 89  auc: 0.7778207275694712\n",
            " epoch: 90  auc: 0.7779495769445518\n",
            " epoch: 91  auc: 0.7781643259030194\n",
            " epoch: 92  auc: 0.7784220246531804\n",
            " epoch: 93  auc: 0.7785938238199545\n",
            " epoch: 94  auc: 0.7788515225701156\n",
            " epoch: 95  auc: 0.778980371945196\n",
            " epoch: 96  auc: 0.7791092213202766\n",
            " epoch: 97  auc: 0.7791521711119701\n",
            " epoch: 98  auc: 0.778808572778422\n",
            " epoch: 99  auc: 0.7788944723618091\n",
            " epoch: 100  auc: 0.7792810204870506\n",
            " epoch: 101  auc: 0.7790233217368896\n",
            " epoch: 102  auc: 0.77915217111197\n",
            " epoch: 103  auc: 0.7792380706953571\n",
            " epoch: 104  auc: 0.7791951209036636\n",
            " epoch: 105  auc: 0.7792380706953571\n",
            " epoch: 106  auc: 0.7791092213202766\n",
            " epoch: 107  auc: 0.7792380706953572\n",
            " epoch: 108  auc: 0.7793239702787441\n",
            " epoch: 109  auc: 0.779066271528583\n",
            " epoch: 110  auc: 0.7790233217368896\n",
            " epoch: 111  auc: 0.7788085727784221\n",
            " epoch: 112  auc: 0.7786797234033415\n",
            " epoch: 113  auc: 0.7787656229867285\n",
            " epoch: 114  auc: 0.778980371945196\n",
            " epoch: 115  auc: 0.7790233217368896\n",
            " epoch: 116  auc: 0.7796675686122921\n",
            " epoch: 117  auc: 0.7798178928832196\n",
            " epoch: 118  auc: 0.7800111669458403\n",
            " epoch: 119  auc: 0.7797534681956793\n",
            " epoch: 120  auc: 0.7799682171541469\n",
            " epoch: 121  auc: 0.7801829661126144\n",
            " epoch: 122  auc: 0.7801400163209208\n",
            " epoch: 123  auc: 0.7797534681956793\n",
            " epoch: 124  auc: 0.7798823175707598\n",
            " epoch: 125  auc: 0.7798393677790663\n",
            " epoch: 126  auc: 0.7795387192372117\n",
            " epoch: 127  auc: 0.7797964179873728\n",
            " epoch: 128  auc: 0.7796675686122922\n",
            " epoch: 129  auc: 0.7793239702787441\n",
            " epoch: 130  auc: 0.7790662715285832\n",
            " epoch: 131  auc: 0.7794098698621312\n",
            " epoch: 132  auc: 0.7794098698621311\n",
            " epoch: 133  auc: 0.7792380706953571\n",
            " epoch: 134  auc: 0.7793669200704376\n",
            " epoch: 135  auc: 0.7795387192372117\n",
            " epoch: 136  auc: 0.7797534681956793\n",
            " epoch: 137  auc: 0.7797964179873726\n",
            " epoch: 138  auc: 0.7795387192372117\n",
            " epoch: 139  auc: 0.7792810204870506\n",
            " epoch: 140  auc: 0.7792810204870506\n",
            " epoch: 141  auc: 0.7793239702787441\n",
            " epoch: 142  auc: 0.7794528196538247\n",
            " epoch: 143  auc: 0.7794528196538247\n",
            " epoch: 144  auc: 0.7797534681956793\n",
            " epoch: 145  auc: 0.7795816690289052\n",
            " epoch: 146  auc: 0.7793669200704377\n",
            " epoch: 147  auc: 0.7792810204870506\n",
            " epoch: 148  auc: 0.7793669200704377\n",
            " epoch: 149  auc: 0.7792380706953571\n",
            " epoch: 150  auc: 0.778894472361809\n",
            " epoch: 151  auc: 0.7789374221535026\n",
            " epoch: 152  auc: 0.7787656229867285\n",
            " epoch: 153  auc: 0.7785938238199545\n",
            " epoch: 154  auc: 0.7785079242365675\n",
            " epoch: 155  auc: 0.7782072756947129\n",
            " epoch: 156  auc: 0.7781213761113258\n",
            " epoch: 157  auc: 0.7779495769445519\n",
            " epoch: 158  auc: 0.7778207275694713\n",
            " epoch: 159  auc: 0.7777348279860843\n",
            " epoch: 160  auc: 0.7776489284026973\n",
            " epoch: 161  auc: 0.7776059786110038\n",
            " epoch: 162  auc: 0.7773912296525362\n",
            " epoch: 163  auc: 0.7773482798608427\n",
            " epoch: 164  auc: 0.7772623802774556\n",
            " epoch: 165  auc: 0.7772194304857621\n",
            " epoch: 166  auc: 0.7771335309023751\n",
            " epoch: 167  auc: 0.7770046815272946\n",
            " epoch: 168  auc: 0.7770905811106816\n",
            " epoch: 169  auc: 0.7768758321522141\n",
            " epoch: 170  auc: 0.776789932568827\n",
            " epoch: 171  auc: 0.7762745350685049\n",
            " epoch: 172  auc: 0.7762315852768114\n",
            " epoch: 173  auc: 0.7760597861100373\n",
            " epoch: 174  auc: 0.7758879869432633\n",
            " epoch: 175  auc: 0.7759309367349568\n",
            " epoch: 176  auc: 0.7759309367349568\n",
            " epoch: 177  auc: 0.7758020873598763\n",
            " epoch: 178  auc: 0.7756302881931022\n",
            " epoch: 179  auc: 0.7756732379847957\n",
            " epoch: 180  auc: 0.7754584890263282\n",
            " epoch: 181  auc: 0.7751578404844736\n",
            " epoch: 182  auc: 0.7749430915260062\n",
            " epoch: 183  auc: 0.7748142421509255\n",
            " epoch: 184  auc: 0.774943091526006\n",
            " epoch: 185  auc: 0.7749860413176997\n",
            " epoch: 186  auc: 0.7746424429841515\n",
            " epoch: 187  auc: 0.774427694025684\n",
            " epoch: 188  auc: 0.7744706438173775\n",
            " epoch: 189  auc: 0.7742558948589099\n",
            " epoch: 190  auc: 0.7742129450672164\n",
            " epoch: 191  auc: 0.7742129450672164\n",
            " epoch: 192  auc: 0.7739552463170554\n",
            " epoch: 193  auc: 0.7736975475668942\n",
            " epoch: 194  auc: 0.7736116479835073\n",
            " epoch: 195  auc: 0.7733968990250397\n",
            " epoch: 196  auc: 0.7729244513164112\n",
            " epoch: 197  auc: 0.7726667525662501\n",
            " epoch: 198  auc: 0.772580852982863\n",
            " epoch: 199  auc: 0.7723661040243954\n",
            " epoch: 200  auc: 0.7723231542327019\n",
            " epoch: 201  auc: 0.7721084052742344\n",
            " epoch: 202  auc: 0.7719366061074604\n",
            " epoch: 203  auc: 0.7717218571489928\n",
            " epoch: 204  auc: 0.7712064596486706\n",
            " epoch: 205  auc: 0.7710346604818967\n",
            " epoch: 206  auc: 0.7708199115234291\n",
            " epoch: 207  auc: 0.7705192629815746\n",
            " epoch: 208  auc: 0.770476313189881\n",
            " epoch: 209  auc: 0.77021861443972\n",
            " epoch: 210  auc: 0.77021861443972\n",
            " epoch: 211  auc: 0.7700897650646394\n",
            " epoch: 212  auc: 0.7700468152729459\n",
            " epoch: 213  auc: 0.7700468152729459\n",
            " epoch: 214  auc: 0.7699609156895588\n",
            " epoch: 215  auc: 0.7698750161061718\n",
            " epoch: 216  auc: 0.7698750161061719\n",
            " epoch: 217  auc: 0.7699179658978654\n",
            " epoch: 218  auc: 0.7697032169393979\n",
            " epoch: 219  auc: 0.7696173173560109\n",
            " epoch: 220  auc: 0.7696602671477043\n",
            " epoch: 221  auc: 0.7697891165227848\n",
            "fold: 5  epoch: 121\n",
            "fold: 5  accuracy: 0.940251572327044\n",
            "fold: 5  f1_score: 0.05\n",
            "fold: 5  auc: 0.7801829661126144\n",
            "###################################################\n",
            "auc: 0.7567361233557084\n",
            "f1 score: 0.02081081081081081\n",
            "accuracy: 0.9424715895066299\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}