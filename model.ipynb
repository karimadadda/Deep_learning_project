{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeLxi5eCjX8AKd1RjwpqML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimadadda/Deep_learning_project/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A69mamObR2oa"
      },
      "outputs": [],
      "source": [
        "\n",
        "from utils import *\n",
        "\n",
        "# AMI-NET\n",
        "class Graph(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, tokens, d_model, feat_max, num_heads, rate):\n",
        "\n",
        "        super(Graph, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(tokens, d_model)\n",
        "        self.multihead_att = MultiHeadAttention(d_model, num_heads)\n",
        "        self.pooling = MIL_gated_attention(feat_max)\n",
        "        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.w1 = tf.keras.layers.Dense(d_model/2, activation='relu')\n",
        "        self.w2 = tf.keras.layers.Dense(d_model/4)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x_bin):\n",
        "\n",
        "        # word embedding\n",
        "        x = self.embedding(x_bin)\n",
        "\n",
        "        # multi-head attention\n",
        "        mha_out, mha_att_matrix = self.multihead_att(x, x, x)\n",
        "        mha_out = self.dropout1(mha_out)\n",
        "        out = self.ln(x + mha_out)\n",
        "\n",
        "        # fully connected layers\n",
        "        x_dense1 = self.w1(out)\n",
        "        x_dense2 = self.w2(x_dense1)\n",
        "        x_dense2_drop = self.dropout2(x_dense2)\n",
        "\n",
        "        # Instance-level Pooling\n",
        "        rep = tf.reduce_sum(x_dense2_drop, axis=-1)\n",
        "\n",
        "        # Bag-level Pooling\n",
        "        mil_out, mil_att_matrix = self.pooling(rep)\n",
        "        pred = tf.nn.sigmoid(mil_out)\n",
        "\n",
        "        return pred, mha_att_matrix, mil_att_matrix\n"
      ]
    }
  ]
}